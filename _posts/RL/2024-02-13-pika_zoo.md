---
layout: single
title: "피카츄 배구 환경만들고 강화학습 학습하기"
date: 2024-02-13 02:00:00
lastmod: 2024-02-13 02:00:00
categories: RL
tag: [RL, pettingzoo]
toc: true
toc_sticky: true
use_math: false
published: false
---

## 좌우 판단할 피쳐를 넣음

Multi-agent에서 좌우를 왔다갔다 할경우 바뀔 때마다 자기가 어느 위치인지 알 요소를 알려줘야함

## f key 옵션을 넣음

observation_space를 통일하기 위한 수단이었음

## Observation 요소 중 필요한 것들은 one_hot

state가 연속적인 feature가 아니기에 one_hot으로 변경함

https://github.com/helpingstar/pika-zoo/commit/3427e59bc83a4468ca1fde26c3d453d4b6acb894

## Observation Space 변경

`MultiBinary(6)` -> `Discrete(2 ** 6)`

## Observation Space 변경 2

obs를 통일, 환경측면에서 그게 맞다 판단. 나중에 직접 플레이하면 변환 함수 구현하면 된다.

2진법으로 키를 구성하면 자기 주장이 센 action이 있다.

`Discrete(2 ** 6)` -> `Discrete(18)`

## 정보 참조는 info로

`env.xxx`로 직접 접근하기보다는 `info` 로 리턴하자

## reward 오류 발견

https://github.com/helpingstar/pika-zoo/commit/025c25b7968e2955d92911a1af7ecbe19b7c050a

## Observation Space 변경 3

무조건 0인 observation 피처 제거 MDP를 위해 필요한 obs추가

https://github.com/helpingstar/pika-zoo/commit/f9adf0f6a72c4892314bfd224196803355acec3e

## Random agent 통계

Random agent 끼리 붙었을 때
* 한 라운드는 평균 46.26 step(=frame)
* 한 게임은 평균 1196.24 step
* 누적 점수 : 49.72 vs 50.27
* 승률 : 50.12 vs 49.87

## 환경 로직 최적화

사람끼리 붙을 시 필요없는 함수실행 제거

https://github.com/helpingstar/pika-zoo/commit/cb485b046837e44d9d9f109bcdc2baf7a270620b

## Wrapper 추가

* 에이전트와 공의 상대적 위치에 따른 보상체계
* Normal State(=모든 step)에 대한 보상 설정

## 환경 Argument 추가

컴퓨터인지 아닌지, 승리 점수 설정가능

## Wrapper 추가 2

기본 Rule Based AI 와 더 편하게 붙기 위해 Gymansium 스타일로 바꿔주는 Wrapper 생성


## Observation Space 변경 4

피쳐 하나 제거

https://github.com/helpingstar/pika-zoo/commit/0a1276f620d376316d27922a61d8b56d2e8c8219

## 환경 Vecotrizing

두가지 방법 존재

* 두 에이전트를 동시에 학습하기
* 기존처럼 agent pool을 만들고 가중치 고정해서 학습하기

