---
layout: single
title: 강화학습 문답
date: 2023-01-19 17:21:57
lastmod : 2023-01-19 17:22:02
categories: RL
tag: [RL]
toc: true
toc_sticky: true
use_math: true
---

유용하다 생각했던 강화학습 관련 Q&A를 정리하는 글이다. 답변자의 신뢰도는 고려하지 않으니 참고할 때 유의하기 바란다.

# 1.

[Loss not decreasing but performance is improving](https://stats.stackexchange.com/q/313876)

(질문 요약)

SpaceInvaders에 DQN을 구현한다. 에피소드의 보상은 지속적으로 증가하여 최대 보상에 근접했다. 하지만 loss는 증가하여 일정한 감소 없이 진동한다. 이것의 이유는 무엇인가.

내 첫 번째 추측은 움직이는 타겟인데 Pong같은 다른 게임(움직이는 타겟이 있는)에서는 loss의 감소가 관측될 수 있는가? learning rate의 감소가 진동을 수정한다는 것은 아마 분명하다. 그러나 나는 이미지의 특정 결과와 손실이 감소하지 않는 경우 어떻게 학습하는지에 관심이 있다.

(답변)

그것은 강화학습에서 특별한 경우가 아닙니다 그것이 어떤 것이 잘못됐다는 것을 의미하지도 않습니다. 에이전트가 더 잘 학습하게 되면 보상을 추정하는 것이 더욱 어려워집니다.(더이상 항상 0이 아니기 때문입니다). 게다가 보상이 높아지고 에피소드의 평균 길이가 길어지면, 보상의 분산 또한 커질 수 있어 손실이 커지는 것을 막는 것조차 쉽지 않습니다. 당신이 언급한 세번째 요소는 끊임없이 변화하는 것이 Q-network에 "moving-target" 문제를 야기한다는 것이다.

(댓글)
'non-stationary'라는 용어는 RL에서 'moving target'대신에 봤던 용어입니다. 정책이 개선되는 동안 정책의 가치는 non-stationary합니다.
