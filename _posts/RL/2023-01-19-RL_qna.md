---
layout: single
title: 강화학습 문답
date: 2023-01-19 17:21:57
lastmod : 2023-05-12 20:42:34
categories: RL
tag: [RL]
toc: true
toc_sticky: true
use_math: true
---

유용하다 생각했던 강화학습 관련 Q&A를 정리하는 글이다. 답변자의 신뢰도는 고려하지 않으니 참고할 때 유의하기 바란다.

# 1.

[Loss not decreasing but performance is improving](https://stats.stackexchange.com/q/313876)

**(질문 요약)**

SpaceInvaders에 DQN을 구현한다. 에피소드의 보상은 지속적으로 증가하여 최대 보상에 근접했다. 하지만 loss는 증가하여 일정한 감소 없이 진동한다. 이것의 이유는 무엇인가.

내 첫 번째 추측은 움직이는 타겟인데 Pong같은 다른 게임(움직이는 타겟이 있는)에서는 loss의 감소가 관측될 수 있는가? learning rate의 감소가 진동을 수정한다는 것은 아마 분명하다. 그러나 나는 이미지의 특정 결과와 손실이 감소하지 않는 경우 어떻게 학습하는지에 관심이 있다.

**(답변)**

그것은 강화학습에서 특별한 경우가 아닙니다 그것이 어떤 것이 잘못됐다는 것을 의미하지도 않습니다. 에이전트가 더 잘 학습하게 되면 보상을 추정하는 것이 더욱 어려워집니다.(더이상 항상 0이 아니기 때문입니다). 게다가 보상이 높아지고 에피소드의 평균 길이가 길어지면, 보상의 분산 또한 커질 수 있어 손실이 커지는 것을 막는 것조차 쉽지 않습니다. 당신이 언급한 세번째 요소는 끊임없이 변화하는 것이 Q-network에 "moving-target" 문제를 야기한다는 것이다.

(댓글)
'non-stationary'라는 용어는 RL에서 'moving target'대신에 봤던 용어입니다. 정책이 개선되는 동안 정책의 가치는 non-stationary합니다.

# 2.

[When are Monte Carlo methods preferred over temporal difference ones?](https://stats.stackexchange.com/q/336974)

# 3.

[What is the credit assignment problem?](https://ai.stackexchange.com/q/12908)

# 4.

[What is predicted and controlled in reinforcement Learning?](https://stats.stackexchange.com/q/340462)

**(질문 요약)**

강화학습에서 몬테카를로 예측(prediction), 몬테카를로 제어(control)와 같이 제어와 예측에 관한 많은 용어를 보았습니다.

그 둘은 정확히 무엇입니까?

**(답변 요약)**

Prediction과 Control의 차이는 정책에 관한 목표와 관련이 있습니다. 정책은 현재 상태에서 행동할 방법을 서술합니다. 이것은 문헌에서 종종 $\pi(a \vert s)$로 표기됩니다. 상태 $s$에서 $a$를 택할 확률입니다.

강화학습에서의 prediction 문제는 정책이 제공되었을 때 그 정책이 얼마나 잘 수행하는지를 측정하는 것을 목표로 합니다. 즉 함수 $\pi(a \vert s)$가 고정되었을 때 어떤 상태에서도 보상의 총량의 기댓값을 예상하는 것입니다.

강화학습에서의 control 문제는 정책이 고정되어 있지 않은 상태에서 최적 정책을 찾는 것을 목표로 합니다. 즉 어떤 상태에서도 reward의 총량의 기댓값을 최대화하는 $\pi(a \vert s)$를 찾는 것입니다.

가치 함수를 기반으로 한 control 알고리즘(ex. 몬테카를로)은 종종 보통 prediction 문제를 품으로써 동작합니다. 즉 다양한 행동의 가치를 predict 하고 각 step에서 가장 좋은 행동을 선택하도록 정책을 조정합니다. 결국 가치함수 기반 알고리즘은 출력은 일반적으로 대략적인 최적 정책과 해당 정책을 따를 때 미래의 보상의 기댓값입니다.
