---
layout: single
title: "(WIP)Mastering the Game of Go with Deep Neural Networks and Tree Search"
date: 2024-01-02 01:47:00
lastmod: 2024-01-02 01:47:00
categories: RL
tag: [RL, Alphago]
toc: true
toc_sticky: true
use_math: true
published: false
---

* fast rollout policy : $p_\pi$
* supervised laerning(SL) policy network : $p_\sigma$
* reinforcement learning(RL) policy network : $p_\rho$
* value network : $v_\theta$

$$
\Delta \sigma \propto \frac{\partial \log p_{\sigma}(a|s)}{\partial \sigma}. \tag{1}
$$

$$
\Delta \rho \propto \frac{\partial \log p_{\rho}(a_t | s_t)}{\partial \rho} z_t. \tag{2}
$$

$$
v^p(s) = \mathbb{E} \left [ z_t | s_t = s, a_{t...T} \sim p \right ]. \tag{3}
$$

We approximate the value function using a value network $v_{\theta}(s)$ with weights $\theta$, $v_{\theta}(s) \approx v^{p_{\rho}}(s) \approx v^{*}(s)$.

$$
\Delta \theta \propto \frac{\partial v_{\theta}(s)}{\partial \theta} (z - v_{\theta}(s)). \tag{4}
$$

$$
a_t = \underset{a}{\text{argmax}}(Q(s_t, a) + u(s_t, a)), \tag{5}
$$

so as to maximize action value plus a bonus

$$
u(s, a) \propto \frac{P(s,a)}{1 + N(s,a)}
$$

$$
V(s_L) = (1-\lambda)v_{\theta}(s_L)+\lambda z_L. \tag{6}
$$

$$
\begin{align*}
    & N(s,a) = \sum_{i=1}^{n}1(s,a,i) \tag{7} \\
    & Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^{n}1(s,a,i)V(s_{L}^{i}) \tag{8}
\end{align*}
$$