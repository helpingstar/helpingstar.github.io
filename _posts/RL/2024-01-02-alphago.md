---
layout: single
title: "(WIP)Mastering the Game of Go with Deep Neural Networks and Tree Search"
date: 2024-01-02 01:47:00
lastmod: 2024-01-02 01:47:00
categories: RL
tag: [RL, Alphago]
toc: true
toc_sticky: true
use_math: true
published: false
---

* fast rollout policy : $p_\pi$
* supervised laerning(SL) policy network : $p_\sigma$
* reinforcement learning(RL) policy network : $p_\rho$
* value network : $v_\theta$

$$
\Delta \sigma \propto \frac{\partial \log p_{\sigma}(a|s)}{\partial \sigma}. \tag{1}
$$

$$
\Delta \rho \propto \frac{\partial \log p_{\rho}(a_t | s_t)}{\partial \rho} z_t. \tag{2}
$$

$$
v^p(s) = \mathbb{E} \left [ z_t | s_t = s, a_{t...T} \sim p \right ]. \tag{3}
$$

We approximate the value function using a value network $v_{\theta}(s)$ with weights $\theta$, $v_{\theta}(s) \approx v^{p_{\rho}}(s) \approx v^{*}(s)$.

$$
\Delta \theta \propto \frac{\partial v_{\theta}(s)}{\partial \theta} (z - v_{\theta}(s)). \tag{4}
$$

$$
a_t = \underset{a}{\text{argmax}}(Q(s_t, a) + u(s_t, a)), \tag{5}
$$

so as to maximize action value plus a bonus

$$
u(s, a) \propto \frac{P(s,a)}{1 + N(s,a)}
$$

$$
V(s_L) = (1-\lambda)v_{\theta}(s_L)+\lambda z_L. \tag{6}
$$

$$
\begin{align*}
    & N(s,a) = \sum_{i=1}^{n}1(s,a,i) \tag{7} \\
    & Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^{n}1(s,a,i)V(s_{L}^{i}) \tag{8}
\end{align*}
$$

## METHODS

### Problem setting

* state space : $\mathcal{S}$
* action space : $\mathcal{A}(s)$
* state : $s \in \mathcal{S}$
* state transition function : $f(s, a, \xi)$
  * defining the successor state after selecting action $a$ in state $s$ and random input $\xi$
* reward function : $r^{i}(s)$
  * the reward received  by player $i$ in state $s$.
* terminal time step : $T$
* outcome of the game : $z_t = \plusmn r(s_T)$
  * terminal reward at the end of the game from the perspective of the current player at time step $t$.
* policy : $p(a|s)$
  * probability distribution over legal actions $a \in \mathcal{A}(s)$
* value function : $v^p(s) = \mathbb{E} \left[ z_t \vert s_t = s, a_{t...T} \sim p \right]$
  * expected outcome if all actions for both players are selected according to policy $p$
* optimal value function : $v^{*}$x

$$
v^*(s) = 
\begin{cases} 
z_T & \text{if } s = s_T, \\
\underset{a}{\max} -v^*(f(s,a)) & \text{otherwise}.
\end{cases}
$$

### Prior work

$v(s) \approx v^{*}(s)$

The majority of prior work has focused on a linear combination $v_{\theta}(s) = \varphi(s) \cdot \theta$ of features $\varphi(s)$ with weights $\theta$.

A alternative approach to minimax search is Monte Carlo tree search (MCTS), which estimates the optimal value of interior nodes by a double approximation, $V^n(s) \approx v^{P^n}(s) \approx v^{*}(s)$.
* $V^n(s) \approx v^{P^n}(s)$ : uses $n$ Monte Carlo simulations to estimate the value function of a simulation policy $P^n$.
* $v^{P^n}(s) \approx v^{*}(s)$ : uses a simuation polciy $P^n$ in place of minimax optimal actions.

The simulation policy selects actions according to a search control function $\argmax_a (Q^n(s,a) + u(s,a))$, such as UCT, that selects children with higher action values, $Q^n(s,a) = -V^n(f(s,a))$, plus a bonus $u(s,a)$ that encourages exploration; or in the absence of a search tree at state $s$, it samples actions from a fast rollout policy $p_\pi(a \vert s)$.

$\lim_{n \rightarrow \infty}V^{n}(s) = \lim_{n \rightarrow \infty}v^{P^n}(s)=v^{*}(s)$.

### Search algorithm

asynchronous policy and value MCTS algorithm (APV-MCTS).
* Each node $s$ in the search tree contains edges $(s,a)$ for all legal actions $a \in \mathcal{A}(s)$.
* Each edge stores a set of statistics,

$$
\{ P(s,a), \hspace{0.5em} N_v(s,a), \hspace{0.5em} N_r(s,a), \hspace{0.5em} W_v(s,a), \hspace{0.5em} W_r(s,a), \hspace{0.5em} Q(s,a) \}
$$

* $P(s,a)$ : prior probability
* $W_v(s,a), W_r(s,a)$ : Monte Carlo estimates of total action value, accumulated over $N_v(s,a)$ and $N_r(s,a)$ leaf evaluations and rollout rewards, respectively.
* $Q(s,a)$ : combined mean action value for that edge.

#### Selection

The first in-tree phase of each simulation begins at the root of the search tree and finishes when the simulation reaches a leaf node at time step $L$. At each of these time steps, $t < L$, an action is selected according to the statistics in the search tree, $a_t = \argmax_a(Q(s_t,a) + u(s_t,a))$ using a variant of the PUCT algorithm, $u(s,a) = c_{\text{puct}}P(s,a) \frac{\sqrt{\sum_b N_r(s,b)}}{1 + N_r(s,a)}$
* $c_{\text{puct}}$ : constant determining the level of exploration

#### Evaluation

#### Backup

At each in-tree step $t \leq L$ of the simulation, the rollout statistics are updated as if it has lost $n_{\text{vl}}$ games, $N_r(s_t, a_t) \leftarrow N_r(s_t, a_t) + n_{\text{vl}}; W_r(s_t, a_t) \leftarrow W_r(s_t, a_t) - n_{\text{vl}}$; this virtual loss discourages other threads from simultaneously exploring the identical variation. At the end of the simulation, the rollout statistics are updated in a backward pass through each step $t \leq L$, replacing the virtual losses by the outcome, $N_r(s_t, a_t) \leftarrow N_r(s_t, a_t) - n_{\text{vl}} + 1; W_r(s_t, a_t) \leftarrow W_r(s_t, a_t) + n_{\text{vl}} + z_t$.

$N_v(s_t, a_t) \leftarrow N_v(s_t, a_t) + 1, W_v(s_t, a_t) \leftarrow W_v(s_t, a_t) + v_{\theta}(s_L)$

$Q(s, a) = (1 - \lambda) \frac{W_v(s, a)}{N_v(s, a)} + \lambda \frac{W_r(s, a)}{N_r(s, a)}.$

#### Expansion

$\{N(s', a) = N_r(s', a) = 0, W(s', a) = W_r(s', a) = 0, P(s',a) = p_{\sigma}(a|s')\}$

### Rollout policy

### Symmetry

$\overline{v}_{\theta}(s)=\frac{1}{8}\sum_{j=1}^8 v_\theta (d_j(s))$

$\overline{p}_{\sigma}(\cdot|s) = \frac{1}{8} \sum_{j=1}^{8} d_j^{-1}(p_{\sigma}(\cdot|d_j(s)))$

### Policy network: classification

$\Delta \sigma = \frac{\alpha}{m} \sum_{k=1}^{m} \frac{\partial \log p_{\sigma}(a^k|s^k)}{\partial \sigma}.$

* step size $\alpha$ was initialized to 0.003 and was halved every 80 million training steps, without momentum terms.
* minibatch size of $m$=16

### Policy network: reinforcement learning

$\Delta \rho = \frac{\alpha}{n} \sum_{i=1}^{n} \sum_{t=1}^{T^i} \frac{\partial \log p_{\rho}(a_t^i | s_t^i)}{\partial \rho} \left( z_t^i - v(s_t^i) \right)$

### Value network: regression

$v^{p_{\rho}}(s_{U+1}) = \mathbb{E}[z_{U+1} | s_{U+1}, a_{U+1,...T} \sim p_{\rho}]$

$\Delta \theta = \frac{\alpha}{m} \sum_{k=1}^{m} (z^k - v_{\theta}(s^k)) \frac{\partial v_{\theta}(s^k)}{\partial \theta}$

### Features for policy/value network

### Neural network architecture

### Evaluation

$p$($a$ beats $b$) $= \frac{1}{1 + \exp(c_{\text{elo}}(e(b) - e(a)))}$
* $c_{\text{elo}} = 1/400$