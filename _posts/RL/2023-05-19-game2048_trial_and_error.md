---
layout: single
title: "2048 게임 강화학습 도전기"
date: 2023-05-19 22:18:12
lastmod : 2023-05-22 12:14:21
categories: RL
tag: [RL, PPO, '2048']
toc: true
toc_sticky: true
use_math: true
---

2048 게임을 정복하기 위해 [gym-game2048](https://github.com/helpingstar/gym-game2048) 강화학습 환경을 만들고 그것을 정복하기 위해 시도한 시행착오를 적어놓은 일지이다.

(2023-05-19 23:43:28)

# 1. sparse reward vs dense reward

2048 게임의 목표는 2048이라는 숫자를 만드는 것이다. 그리고 점수는 합쳐진 숫자들의 합이다. 예를 들어 4와 4를 합쳐서 8을 만들었다면 8점이 추가된다. 또 2와 2, 4와 4를 합쳤다면 12점이 올라가게 된다. 여기서 보상을 어떻게 설정할 것인가에 대해 고민이 있었다.

게임의 목표를 2048을 최대한 빨리 만드는 것으로 한다면 점수를 보상으로 하였을 때 게임을 바로 끝내지 않고 더 점수를 얻고 끝내는 상황이 나올수 있다는 생각이 들었다. 또한 예를 들어서

```
# ex1
2 4 2 4
0 0 0 0
0 0 0 0
0 0 0 0
```

와 같은 상황에서 위로 올리면 블록이 새로 스폰되지도 않고 아무일도 일어나지 않으나 이것은 한 스텝에 해당한다. 이럴 경우 게임 진행이 거의 멈춰버릴 수 있다. 이를 방지하기 위해 모든 스텝에 대해 -0.001의 보상을 주고 학습을 했다. 그리고 실패, 성공 각각에 대해 -5, +5의 보상을 주고 학습을 했다.

결과는 실패였다. 원인은 다음과 같다. 스텝의 수가 일정 횟수를 넘어가면 그것이 실패의 보상인 -5보다 낮아진다. 그럴 경우 에이전트는 "기약없는 무의미한 행동을 반복할 바에는 빨리 죽어서 보상을 최대화하자" 라고 생각하게 된다.

# 2. truncated의 주의점

1번과 같은 이유로 `TimeLimit`을 이용해서 일정 횟수를 넘어가면 truncate를 했다. 이것은 무행동을 반복하고 있을 확률이 높기 때문에 그전에 끊어버리는 것이다. 이러면 경우에 따라 문제가 각각 있었다.
1. 시간제한 × (step의 보상) 이 클리어 실패시 보상보다 높을 경우 (예를 들어 -0.0001 × 30000 > -5) 학습이 되지 않는다. 지정된 시간 내에 빨리 클리어를 해서 보상을 높이라는 의도였으나. 실패할바에 그때까지 최대한 버텨서 보상을 높이는 것이다.
2. 시간제한 × (steps의 보상) 이 클리어 실패시 보상보다 낮을 경우 다른 관점으로 문제가 발생하는데, 스텝이 너무 높아지면 이득이 너무 낮아지므로 그 전에 최대한 빨리 게임을 끝내버리려고 하는 것이다.

TimeLimit가 효과가 없던 것은 결국 양의 보상이 너무 희소하기 때문에 벌어진 일이었다.

# 3. 보상 설계 변경

보상을 게임의 게임의 점수 체계와 같게 하였다. 합쳐진 블록에 각각 $\log_2$를 취하고 0.1을 곱해서 모두 더하였다. 예를 들어 4+4, 2+2가 동시에 합쳐졌다면, 8, 4가 만들어지므로 0.3, 0.2를 더해서 0.5의 보상을 받는 것이다. 로그를 취하는 것이 맞는것일까 라는 생각을 했다. 2048을 만들면 나오는 1.1과 1024를 만들면 나오는 1.0의 보상의 가치가 같을까 하는 생각이 든다. 게다가 2048을 만들면 게임이 끝나버려서 더이상의 보상을 어디 못하게 된다.

# 4. 한계
![gym-game2048-1](../../assets/images/rl/gym_game2048/gym-game2048-1.png){: width="80%" height="80%" class="align-center"}

<p style="text-align: center; font-style: italic;"> (Exponential Moving Average: 0.99) </p>

 전체적으로 상승곡선을 그리기 때문에 더 시간을 투자한다면 해결될 수도 있지만, 여기까지에 대해 세가지 개선 방안을 생각해 보았다.

1. (16, 64), (64, 64), (64, ~) 로 설계된 actor, critic 신경망의 유닛 개수를 늘려 보기
2. CNN을 사용해서 공간 정보를 더 얻어보기, 예를 들어 위 아래는 게임적으로 연관이 있으나 Flatten 되면서 공간 정보가 사라지게 된다.
3. 마지막 클리어에 대한 보상을 더 높이기, 현재는 보상이 선형적인데 때문에 최종 보상에 대한 모티브가 부족한게 아닐까 싶다. 1, 2를 현재 실험결과와 비교해본 뒤에 시도해볼 생각이다.

(2023-05-22 12:14:21)

# 5. CNN의 적용

모델을 개선하기 위해 CNN을 적용시키기로 했다. 그와 함께 몇가지를 수정하였다. 수정사항은 다음과 같다.

**기존**

1. MLP만 사용한다.
2. **(1, 4, 4)** ⇒ *Flatten* ⇒ **(16, )** ⇒ *(16, 64)* ⇒ **(64, )** ⇒ *(64, actor/critic)* ⇒ **(actor/critic)**
3. 활성화함수로 Tanh 사용

**변경**

1. CNN도 사용한다.
2. **(1, 4, 4)** ⇒ *(32, 3, 3) kernel* ⇒ **(32, 2, 2)** ⇒ *Flatten* ⇒ **(128, )** ⇒ *(128, actor/critic)* ⇒ **(actor/critic)**
3. 활성화 함수로 ReLU 사용

결과는 다음과 같다. 회색 그래프가 CNN이 적용된 모델이다. 결과 그래프를 보기 전에 특이(원하지 않았던)현상을 먼저 살펴보자

![gym-game2048-6](../../assets/images/rl/gym_game2048/gym-game2048-6.png){: width="80%" height="80%" class="align-center"}

새로운 모델 적용 과정에서 한 특정 에피소드가 **턱에 걸리는(필자가 만들어낸 표현, 아무의미 없는 행동을 많이 반복하는) 현상이 발생했다.** 이전 모델에도 그런 부분이 있긴 했으나 지금만큼 심하지는 않았다. 이로 인해 한 에피소드에서 상당한 양의 스텝을 소비하게 되었다. 이로 인해서 그래프가 약간 이상해졌는데 이에 대해서는 뒤에 자세히 서술하겠다.

그리고 단순 턱에 걸리는 현상 때문인지 성능 향상으로 인해서 한 에피소드에 소비되는 스텝의 양이 증가했기 때문인지 모르겠지만 그로 인하여 아래와 같이 업데이트당 소모하는 스텝이 늘면서 아래와 같은 그래프가 나왔다.

![gym-game2048-2](../../assets/images/rl/gym_game2048/gym-game2048-2.png){: width="80%" height="80%" class="align-center"}

그로 인하여 회색 그래프가 중간에 끊겼다. 아래는 결과이다. (exponential Smoothing: 0.99)

![gym-game2048-4](../../assets/images/rl/gym_game2048/gym-game2048-4.png){: width="80%" height="80%" class="align-center"}

![gym-game2048-3](../../assets/images/rl/gym_game2048/gym-game2048-3.png){: width="80%" height="80%" class="align-center"}

상기해보면 우리의 목표는 max_number를 최대한 크게 만드는 것이었고 그를 위해 보상을 합쳐져서 만들어진 블록에 $\log_2$를 취하고 0.1을 곱하는 것이었다. 그리하여 보상은 [0.2, 0.3, ..., 1.1]을 가지게 된다.
초반 추세는 회색 그래프가 빠르지만 마무리 상태에서는 비슷하다. 무의미한 행동에 많은 스텝을 소모하게 되면서 제대로된 비교는 못하게 되었다. 하지만 여기까지의 결과에 대해 몇가지 생각을 했다. 엄밀하지는 않다.

1. 늘어난 가중치 때문인지, CNN 때문인지는 확실하지는 않지만 성능의 상승을 주는 것은 맞는 것으로 보인다. 이후의 실험에서는 CNN을 사용하기로 했다.
2. 턱에 걸리는 상황을 피해야 한다. 턱에걸리는 상황은 주로 다음과 같은 상황에서 나오게 된다.

```
# ex2
128 4  2  0
252 2  16 0
128 8  2  0
16  32 4  0
```
에이전트는 한쪽(방향은 무관한 것 같다)으로 높은 숫자를 몰아높고 하나씩 몰아주면서 점점 늘려가는 방식을 취하는데 왼쪽으로 몰아놓았다고 가정할 때 에이전트가 선호하는 방향은 왼쪽, 위쪽, 아래쪽일 것이다. 근데 위와 같은 상황에서는 왼쪽, 위, 아래가 의미없는(행동을 해도 아무런 효과가 없는) 행동이 된다. 의미있는 행동은 오른쪽으로 가는 행동인데 이것은 왼쪽에 이쁘게 모여있는 높은 숫자들의 대형을 흐트리게 된다. 높아진 숫자는 다시 모서리로 넣기 힘들다. 사람 입장에서도 기피하고 싶은 이 상황은 에이전트에게도 확률이 아주 낮게 설정되어 있어 벗어나기 아주 힘든 상태가 된다.

이런 2번같은 상황을 벗어나기 위해 해결책을 생각해보았다.

# 6. 턱에 걸린상태에서 넘어가기

턱에서 넘어가지 않는 상황에 대해 다음과 같은 생각을 해 보았다.
1. 장기적으로는 그것이 보상이 별로라는 사실에 대해 알기 때문에 장기적으로는 기대보상이 0인 상황을 벗어나기 위해 바로 움직이게 될 것이다.
2. 턱에서 벗어나려는 동기가 부족할 수도 있다. 벗어나려면 이쁘지 않지만 어찌됐든 그 상태를 벗어나야 한다. 어차피 가많이 있어봤자 보상은 0이기 때문이다. ex2로 생각하면 오른쪽으로 옮겨야 하는 것이다. 그런데 2와 2를 합쳐서 보상을 0.2를 얻는 것과 256과 256을 합쳐서 0.8의 보상을 얻는 것이 거의 차이가 나지 않으니 더 큰 숫자를 합치는 것에 대해 동기가 부족한게 아닐까 하는 생각을 했다.
3. 일반적인 행동에 대해 음의 보상을 주는것은 어떨까, 성공, 실패만을 보상 설정에 이용하는 경우(보상의 희소했던 경우) step에 대해 음의 보상을 주는 것은 역효과가 있었지만, 지금과 같이 나름 학습이 잘 되는 경우 턱에 걸린 상황에서 음의 보상을 주는 것은 효과가 있을 수도 있겠다는 생각이 들었다.
4. 시간 제한(TimeLimit)을 두는 것이다. 최악의 상황에서 모든 블록이 2로 스폰되었고 16칸을 모두 1024로 만들 때 무의미한 행동(아무 블럭도 움직이지 않는 행동)을 안 한다고 가정하면 1024 * 16 / 2 = 8192번의 행동이 필요하다. 그런데 심한 경우 에피소드의 길이가 10만 단위까지 넘어가기도 한다. 이럴 경우 시간 제한을 통해 truncation으로 다음 학습을 도모하는 것도 나쁘지 않을 것 같다는 생각도 든다. 하지만 그런 행동에 대해 진짜 얼마나 나쁜지 확인하기 전에 학습하기 전에 끊어버릴경우 그 것이 얼마나 나쁜지를 영영 모를 수도 있다느 생각이 들었다.

# 7. 보상체계 변경

[**6 턱에 걸린상태에서 넘어가기**](#6-턱에-걸린상태에서-넘어가기)의 3번과 관련해서 과연 512 두개를 합쳐서 얻는 1($0.1 \times \log_{2}1024$)의 보상이 2 두개를 합쳐서 얻는 0.2($0.1 \times \log_{2} 4$)의 5배라는 것이 타당한가라는 생각이 들었다. 숫자 4를 5번 만드는 것의 보상이 1024를 만드는 것과 같다. 하지만 이를 만드는데 드는 행동은 약 2 × 5 vs 512로 약 50배가 차이난다.

보상에 log를 취했던 목적은 보상의 범위를 좁히기 위함이었다. 보상의 추세가 $2^x$를 따라가니 보상의 큰 값(또는 분산)이 학습을 어렵게 하지 않을까라는 나의 추측 때문이었다.

그래서 각 보상의 체계를 노력의 정도와 같게 했다. 각 보상의 다음 단계보상까지의 행동의 횟수는 $2^n$ 의 추세를 따라가므로 보상도 그렇게 바꾸었다. 이는 결국 만든 숫자의 추세(4, 8, 16, ...)와 같다. 그리하여 보상쳬계를 다음과 같이 바꾸었다.

보상 체계 변경은 [`RewardByScore`](https://github.com/helpingstar/gym-game2048/blob/main/gym_game2048/wrappers/reward_by_score.py) wrapper를 이용하였다.

합쳐서 만들어진 숫자들을 $\\{c_1, c_2, ..., c_n\\} \in C$ 라고하자, 합쳐진 것이 없을 경우에 보상은 둘다 0이다.

**기존**

$$R = 0.1 \times \sum_{i}^{n}\log_{2}c_i$$
* 범위 : $\left [ 0, 0.1 \times \log_{2}(\text{goal}) \right ]$
* 예시(goal=2048) : 0, 0.1, 0.2, ..., 1.1

**변경**

$$R = 0.01 \times \sum_{i}^{n}c_i$$
* 범위 : $\left [ 0, 0.01 \times \text{goal} \right ]$
* 예시(goal=2048) : 0, 0.02, 0.04, ... 20.48
