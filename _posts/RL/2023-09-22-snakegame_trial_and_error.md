---
layout: single
title: "snakegame 강화학습 도전기"
date: 2023-09-22 20:06:21
lastmod : 2023-09-22 20:06:21
categories: RL
tag: [RL, PPO, snakegame]
toc: true
toc_sticky: true
use_math: true
---

스네이크 게임은 정해진 그리드 격자 배열 안에서 뱀을 움직이면서 1개 이상의 아이템을 먹으면서 몸을 키우는 게임이다. 아이템을 먹으면 몸이 1칸 늘어나게 된다. 자기 몸에 박거나 벽에 박게 되면 게임이 종료된다.

구글에서 **snake game**이라고 검색하면 플레이해볼 수 있다.

이전에 이 문제를 해결하기 위한 여러 시도가 있었다.

1. 격자의 크기를 15로 한 후 DQN으로 문제를 풀려고 시도했지만 실패하였다. 보드의 크기가 크기 때문에 보상을 얻기까지 너무 힘들어 보상의 너무 희소했기 때문인 것 같다.
2. 그래서 격자의 크기를 5로 한 후 문제를 풀어서 성공했었다.

하지만 이제 크기 15에 대해 도전해보려고 한다.

# 0. 초기 설정

* 보상 디자인
  * 아이템을 먹을 때마다 보상을 준다.
  * 죽을 때마다 음의 보상을 준다.
* observation : (15, 15) 크기의 격자
* 알고리즘 : PPO

# 1. 희소한 보상 문제

병렬 환경을 4~6개로 늘렸기 때문에 동시에 많은 환경을 탐색하기에 보상을 금방 학습할 것이라 예상했으나 실패하였다.

1. 양의 보상은 들어오지 않고 죽거나 음의 보상만 주어지니 0의 보상을 받는 행동을 하기 위해 같은 자리를 계속 멤돈다.
2. 이러한 행위를 방지하기 위해 그냥 움직이는 행동에 대해 약한 음의 보상을 주었다. 그랬더니 음의 보상의 누적보다 죽는게 낫겠다 싶어서 죽어버린다.
3. 이럴 경우 에피소드가 아예 끝나지 않는 현상이 발생하여 TimeLimit 을 10000으로 조정하였다. 격자의 개수인 225개에 대한 짐작된 숫자이다.
4. 그냥 죽을 경우 보상을 -10, 의미 없는 움직임일 경우 -0.001 이래도 TimeLimit을 꽉 채워서 죽는 발생하였다. Truncation으로 에피소드가 끝날 경우 다음 보상이 계산된다는 것과 discount factor에 의해 TD Target이 더 작기 때문일 것이다.

# 2. 렌더림 메모리 문제

학습중에 메모리가 치솟으며 중간에 학습이 꺼지는 현상이 발생하였다.

![gym-snakegame-1](../../assets/images/rl/gym_snakegame/gym-snakegame-1.png){: width="80%" height="80%" class="align-center"}

![gym-snakegame-1](../../assets/images/rl/gym_snakegame/gym-snakegame-2.png){: width="80%" height="80%" class="align-center"}

메모리 사용량이 치솟더니 함께 학습이 종료되었다.

오픈소스 CleanRL을 사용했기 때문에 코드 자체에는 문제가 없다고 생각하고 일단 환경에 문제가 있지 않을 까 생각했다.

가장 유력한 후보인 렌더링 문제가 아닐까 생각하였고 200 에피소드마다 에피소드를 동영상으로 렌더링하는데 이 문제가 아닐까 싶었다. 게다가 이 영상을 wandb로 전송하니 말이다.

렌더링을 켰을 경우 모두 같은 문제가 생겼는데 일단 렌더링 옵션을 꺼놓고 (영상 출력이 안되게 해놓고) 학습을 해보았는데 나머지를 동일하게 한 결과 문제가 발생하지 않아 문제가 렌더링 문제가 아닐까 가정하였다.

그리고 고의로 절대 에피소드가 끝나지 않도록 에이전트를 설계하고 실험하였다.

![gym-snakegame-3](../../assets/images/rl/gym_snakegame/gym-snakegame-3.png){: width="80%" height="80%" class="align-center"}

결과는 예상대로였다. 메모리가 치솟고 프로그램이 종료된다.

실험 결과를 통해 파라미터나 보상 전략을 수정하려고 하였으나 여러 문제가 있었다. 이에 대해 몇가지 대처사항이 있다.

1. FPS를 높여서 영상의 크기를 줄인다.
2. 렌더링을 하지 않는다.
3. 무한루프를 벗어나도록 에이전트를 디자인한다.

근데 지금은 일단 실험의 안전성을 위해 당분간 렌더링을 하지 않고 실험 결과는 에피소드의 길이로 추론해보기로 했다.