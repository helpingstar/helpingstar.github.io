---
layout: single
title: "(WIP)DPG, Deterministic Policy Gradient Algorithms"
date: 2024-01-04 20:22:00
lastmod: 2024-01-04 20:22:00
categories: RL
tag: [RL, DPG]
toc: true
toc_sticky: true
use_math: true
published: false
---

## 2. Background

### 2.1. Preliminaries

* Markov decision process(MDP) satisfying the Markov Property $p(s_{t+1} \vert s_1, a_1, ..., s_t, a_t)
 = p(s_{t+1} \vert s_t, a_t)$ for any trajectory $s_1, a_1, s_2, a_2, ..., s_T, a_T$ in state-action space, and a reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$.
  * $\mathcal{S}$ : state space
  * $\mathcal{A}$ : action space
  * $p_1(s_1)$ : initial state distribution with density
  * $p(s_{t+1} \vert s_t, a_t)$ : stationary transition dynamics distribution with conditional density
* $\pi_\theta : \mathcal{S} \rightarrow \mathcal{P(A)}$ : policy is used to select actions in the MDP. In general, the policy is stochastic.
  * $\mathcal{P(A)}$ : set of probability measures on $\mathcal{A}$
  * $\theta \in \mathbb{R}^n$ : vector of $n$ parameters
  * $\pi_\theta(a_t \vert s_t)$ : conditional probability density at $a_t$ associated with the policy
* $h_{1:T} = s_1, a_1, r_1, ..., s_T, a_T, r_T$ over $\mathcal{S} \times \mathcal{A} \times \mathbb{R}$ : trajectory of states, actions, and rewards
* $r_t^{\gamma}=\sum_{k=t}^{\infty} \gamma^{k-t}r(s_k, a_k)$ where $0 < \gamma < 1$ : return, total discounted reward from time-step $t$
* Value functions are defined to be the expected total discounted reward
  * $V^{\pi}(s) = \mathbb{E}[r_1^{\gamma} \vert S_1 = s; \pi]$
  * $Q^{\pi}(s,a)=\mathbb{E}[r_1^{\gamma} \vert S_1 = s, A_1 = a; \pi]$
* $J(\pi) = \mathbb{E} \left[ r_1^{\gamma} \vert \pi \right]$ : performance objective
* $p(s \rightarrow s', t, \pi)$ : density at state $s'$ after transitioning for $t$ time steps from state $s$
* $\rho^\pi(s') := \int_{\mathcal{S}} \sum_{t=1}^{\infty} \gamma^{t-1} p_1(s) p(s \rightarrow s', t, \pi) \text{d}s$ : (improper) discounted state distribution

$$
\begin{align*}
    J(\pi_{\theta}) & = \int_{\mathcal{S}} \rho^{\pi}(s) \int_{\mathcal{A}} \pi_{\theta}(s,a) r(s,a) \text{d}a \text{d}s \\ 
    & = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} [r(s,a)]
\end{align*}\tag{1}
$$

* $\mathbb{E}_{s \sim \rho}[ \cdot ]$ : (improper) expected value with respect to discounted state distribution $\rho(s)$

### 2.2. Stochastic Policy Gradient Theorem

$$
\begin{align*}
    \nabla_{\theta} J(\pi_{\theta}) & = \int_{\mathcal{S}} \rho^{\pi}(s) \int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a|s)Q^{\pi}(s,a) \text{d}a \text{d}s \\
    & = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s)Q^{\pi}(s,a) \right]
\end{align*} \tag{2}
$$

### 2.3. Stochastic Actor-Critic Algorithms

In general, substituting a function approximator $Q^w(s, a)$ for the true action-value function $Q^Ï€(s, a)$ may introduce bias. However, if the function approximator is compatible such that

* i) $Q^w(s,a) = \nabla_{\theta} \log \pi_{\theta}(a|s)^\top w$
  * compatible function approximators are linear in "features" of the stochastic policy, $\nabla_{\theta} \log \pi_{\theta}(a|s)$
* ii) the parameters $w$ are chosen to minimise the mean-squared error $\epsilon^2(w) = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ (Q^w(s, a) - Q^{\pi}(s, a))^2 \right]$
  * requires that the parameters are the solution to the linear regression problem that estimates $Q^{\pi}(s,a)$ from these features.
  * In practice, usually relaxed in favor of policy evaluation algorithms that estimate the value function more efficiently by temporal-difference learning;
there is no bias

$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) Q^w(s, a) \right] \tag{3}
$$

If both i) and i)) are satisfied then the overall algorithm is equivalent to not using a critic at all much like the REINFORCE algorithm.

### 2.4. Off-Policy Actor-Critic

$$
\begin{align*}
J_{\beta}(\pi_{\theta}) & = \int_{\mathcal{S}} \rho^{\beta}(s) V^{\pi}(s) \text{d}s \\
&= \int_{\mathcal{S}} \int_{\mathcal{A}} \rho^{\beta}(s) \pi_{\theta}(a|s) Q^{\pi}(s, a) \text{d}a \text{d}s
\end{align*}
$$

$$
\begin{align*}
  \nabla_{\beta} J_{\beta}(\pi_{\theta}) & \approx \int_{\mathcal{S}} \int_{\mathcal{A}} \rho^{\beta}(s) \nabla_{\theta} \pi_{\theta}(a|s)Q^{\pi}(s, a) \text{d}a \text{d}s \tag{4} \\ 
  & = \mathbb{E}_{s \sim \rho^{\beta}, a \sim \beta} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\beta}(a|s)} \nabla_{\theta} \log \pi_{\theta}(a|s)Q^{\pi}(s, a) \right] \tag{5}
\end{align*}
$$

$\delta_t = r_{t+1} + \gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_t)$

## 3. Gradients of Deterministic Policies

### 3.1. Action-Value Gradients

* Policy evaluation methods estimate the action-value function $Q^\pi(s,a)$ or $Q^\mu (s,a)$
  * ex) Monte-Carlo evaluation or temporal-difference learning.
* Policy improvement methods update the policy with respect to the (estimated) action-value function.
  * The most common approach is a greedy(soft) maximisation of the action-value function $\mu^{k+1}(s) = \underset{a}{\text{argmax}}Q^{\mu^k}(s,a)$.

Instead, a simple and computationally attractive alternative is to move the policy in the direction of the gradient of $Q$, rather than globally maximising $Q$. Specifically, for each visited state $s$, the policy parameters $\theta^{k+1}$ are updated in proportion to the gradient $\nabla_{\theta} Q^{\mu^k}(s, \mu_{\theta}(s))$. Each state suggests a different direction of policy improvement; these may be averaged together by taking an expectation with respect to the state distribution $\rho^{\mu}(s)$,

$$
\theta^{k+1} = \theta^k + \alpha \mathbb{E}_{s \sim \rho^{\mu^k}} \left[ \nabla_{\theta} Q^{\mu^k}(s, \mu_{\theta}(s)) \right] \tag{6}
$$

$$
\theta^{k+1} = \theta^k + \alpha \mathbb{E}_{s \sim \rho^{\mu^k}} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu^k}(s, a) |_{a=\mu_{\theta}(s)} \right] \tag{7}
$$

* $\nabla_{\theta} \mu_{\theta}(s)$ : Jacobian matrix such that each column is the gradient $\nabla_{\theta}[\mu_{\theta}(s)]_d$ of the $d$th action dimension fo the policy with respect to the policy parameters $\theta$

### 3.2. Deterministic Policy Gradient Theorem

* $\mu_{\theta} : \mathcal{S} \rightarrow \mathcal{A}$ : deterministic policy
  * $\theta \in \mathbb{R}$ : parameter vector
* $J(\mu_\theta) = \mathbb{E}[r_1^\gamma \vert \mu]$
* $p(s \rightarrow s', t, \mu)$ : probability distribution
* $\rho^\mu(s)$ : discounted state distribution

This again let us to wrtie the performance objective as an expectation,

$$
\begin{align*}
J(\mu_{\theta}) & = \int_{\mathcal{S}} \rho^{\mu}(s) r(s, \mu_{\theta}(s)) \text{d}s \\ 
& = \mathbb{E}_{s \sim \rho^{\mu}} [r(s, \mu_{\theta}(s))]
\end{align*}
$$

#### Theorem 1

*(Deterministic Policy Gradient Theorem).*
*Suppose that the MDP satisfies conditions A.1 (see Appendix) these imply that $\nabla_{\theta}\mu_{\theta}$ and $\nabla_a Q^\mu(s,a)$ exist and that the deterministic policy gradient exists. Then,*

$$
\begin{align*}
  \nabla_{\theta} J(\mu_{\theta}) & = \int_{\mathcal{S}} \rho^{\mu}(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) |_{a=\mu_{\theta}(s)} \text{d}s \\ & = \mathbb{E}_{s \sim \rho^{\mu}} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) |_{a=\mu_{\theta}(s)} \right]
\end{align*} \tag{9}
$$

### Limit of the Stochastic Policy gradient

We parameterise stochastic policies $\pi_{\mu_\theta, \sigma}$ by a deterministic policy $\mu_\theta : \mathcal{S} \rightarrow \mathcal{A}$ and a variance parameter $\sigma$, such that for $\sigma = 0$ the stochastic policy is equivalent to the deterministic policy, $\pi_{\mu_{\theta}, 0} \equiv \mu_\theta$.

#### Theorem 2.

*Consider a stochastic policy $\pi_{\mu_\theta, \sigma}$ such that $\pi_{\mu_{\theta}, \sigma}(a|s) = \nu_{\sigma}(\mu_{\theta}(s), a)$, where $\sigma$ is a paratmer controlling the variance and $\nu_\sigma$ satisfy conditions B.1 and the MDP satisfies conditions A.1 and A.2. Then,*

$$
\lim_{\sigma \downarrow 0} \nabla_{\theta} J(\pi_{\mu_{\theta}, \sigma}) = \nabla_{\theta} J(\mu_{\theta}) \tag{10}
$$

*where on the l.h.s. the gradient is the standard stochastic policy gradient and on the r.h.s. the gradient is the deterministic policy gradient.*

## 4. Deterministic Actor-Critic Algorithms

### 4.1. On-Policy Deterministic Actor-Critic

* $Q^\mu (s,a)$ -> $Q^w(s,a)$
  * A critic estimates the action-value function $Q^w(s,a) \approx Q^\mu (s,a)$

For example, in the following deterministic actor-critic algorithm, the critic uses Sarsa updates to estimate the action-value function,

$$
\begin{align*}
  \delta_t & = r_t + \gamma Q^w(s_{t+1}, a_{t+1}) - Q^w(s_t, a_t) \tag{11} \\
w_{t+1} & = w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t) \tag{12} \\
\theta_{t+1} & = \theta_t + \alpha_{\theta} \nabla_{\theta} \mu_{\theta}(s_t) \nabla_a Q^w(s_t, a_t)\vert_{a=\mu_{\theta}(s)} \tag{13}
\end{align*}
$$

### 4.2. Off-Policy Deterministic Actor-Critic

* $\mu_\theta(s)$ : determinisitc target policy
* $\pi(s,a)$ : arbitrary stochastic behavior policy

$$
\begin{align*}
  J_{\beta}(\mu_{\theta}) & = \int_{\mathcal{S}} \rho^{\beta}(s) V^{\mu}(s) \text{d}s \\ 
  &= \int_{\mathcal{S}} \rho^{\beta}(s) Q^{\mu}(s, \mu_{\theta}(s)) \text{d}s
\end{align*} \tag{14}
$$

$$
\begin{align*}
  \nabla_{\theta} J_{\beta}(\mu_{\theta}) & \approx \int_{\mathcal{S}} \rho^{\beta}(s) \nabla_{\theta} \mu_{\theta}(a|s)Q^{\mu}(s, a) \text{d}s \\
  & = \mathbb{E}_{s \sim \rho^{\beta}} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a) |_{a=\mu_{\theta}(s)} \right]
\end{align*} \tag{15}
$$

This equation gives the *off-policy deterministic policy gradient*. Analogous to the stochastic case (see Equation 4.)

$$
\begin{align*}
  \nabla_{\beta} J_{\beta}(\pi_{\theta}) & \approx \int_{\mathcal{S}} \int_{\mathcal{A}} \rho^{\beta}(s) \nabla_{\theta} \pi_{\theta}(a|s)Q^{\pi}(s, a) \text{d}a \text{d}s \tag{4} \\ 
  & = \mathbb{E}_{s \sim \rho^{\beta}, a \sim \beta} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\beta}(a|s)} \nabla_{\theta} \log \pi_{\theta}(a|s)Q^{\pi}(s, a) \right] \tag{5}
\end{align*}
$$

we have dropped a term that depends on $\nabla_{\theta}Q^{\mu_\theta}(s,a)$; justification similar to Degris et al. (2012b) can be made in
support of this approximation.

* $Q^\mu (s,a)$ -> $Q^w(s,a)$
  * A critic estimates the action-value function $Q^w(s,a) \approx Q^\mu (s,a)$ off-policy from trajectories generated by $\beta(a \vert s)$

In the following *off-policy determinisitc actor-critic* algorithm, the critic uses Q-learning updates to estimate the action-value function.

$$
\begin{align*}
  \delta_t & = r_t + \gamma Q^w(s_{t+1}, \mu_{\theta}(s_{t+1})) - Q^w(s_t, a_t) \tag{16} \\
w_{t+1} & = w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t) \tag{17} \\
\theta_{t+1} & = \theta_t + \alpha_{\theta} \nabla_{\theta} \mu_{\theta}(s_t) \nabla_a Q^w(s_t, a_t)|_{a=\mu_{\theta}(s)} \tag{18} 
\end{align*}
$$

### 4.3. Compatible Function Approximation

The following theorem applies to both on-policy, $\mathbb{E}[\cdot] = \mathbb{E}_{s \sim \rho^{\mu}}[\cdot]$, and off-policy, $\mathbb{E}[\cdot] = \mathbb{E}_{s \sim \rho^{\beta}}[\cdot]$

#### Theroem 3.

*A function approximator $Q^w(s,a)$ is compatible with a deterministic policy $\mu_{\theta}(s)$, $\nabla_\theta J_\beta(\theta) = \mathbb{E} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^w(s, a)|_{a=\mu_{\theta}(s)} \right]$, if*

1. $\nabla_a Q^w(s, a)|_{a=\mu_{\theta}(s)} = \nabla_{\theta} \mu_{\theta}(s)^\top w$ *and*
2. $w$ *minimises the mean-squared error,* $MSE(\theta, w) = \mathbb{E} \left[ \epsilon(s;\theta,w)^\top \epsilon (s; \theta, w) \right]$
   1. *where* $\epsilon(s; \theta, w) = \nabla_a Q^w(s, a)|_{a=\mu_{\theta}(s)} - \nabla_a Q^{\mu}(s, a)|_{a=\mu_{\theta}(s)}$

#### Proof.

If $w$ minimises the MSE then the gradient of $\epsilon^2$ w.r.t. $w$ must be zero. We then use the fact that, by condition 1, $\nabla_w \epsilon(s; \theta, w) = \nabla_\theta \mu_\theta(s),$

$$
\begin{align*}
\nabla_w \text{MSE}(\theta, w) & = 0 \\
\mathbb{E} \left[ \nabla_{\theta} \mu_{\theta}(s) \epsilon(s;\theta,w) \right] & = 0 \\
\mathbb{E} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^w(s, a)|_{a=\mu_{\theta}(s)} \right] & \\  = \mathbb{E} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^{\mu}(s, a)|_{a=\mu_{\theta}(s)} \right] \\ = \nabla_{\theta} J_{\beta}(\mu_{\theta}) \text{ or } \nabla_{\theta} J(\mu_{\theta})
\end{align*}
$$