---
layout: single
title: "(WIP)DPG, Deterministic Policy Gradient Algorithms"
date: 2024-01-04 20:22:00
lastmod: 2024-01-04 20:22:00
categories: RL
tag: [RL, DPG]
toc: true
toc_sticky: true
use_math: true
# published: true
---

## 2. Background

### 2.1. Preliminaries

* Markov decision process(MDP) satisfying the Markov Property $p(s_{t+1} \vert s_1, a_1, ..., s_t, a_t)
 = p(s_{t+1} \vert s_t, a_t)$ for any trajectory $s_1, a_1, s_2, a_2, ..., s_T, a_T$ in state-action space, and a reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$.
  * $\mathcal{S}$ : state space
  * $\mathcal{A}$ : action space
  * $p_1(s_1)$ : initial state distribution with density
  * $p(s_{t+1} \vert s_t, a_t)$ : stationary transition dynamics distribution with conditional density
* $\pi_\theta : \mathcal{S} \rightarrow \mathcal{P(A)}$ : policy is used to select actions in the MDP. In general, the policy is stochastic.
  * $\mathcal{P(A)}$ : set of probability measures on $\mathcal{A}$
  * $\theta \in \mathbb{R}^n$ : vector of $n$ parameters
  * $\pi_\theta(a_t \vert s_t)$ : conditional probability density at $a_t$ associated with the policy
* $h_{1:T} = s_1, a_1, r_1, ..., s_T, a_T, r_T$ over $\mathcal{S} \times \mathcal{A} \times \mathbb{R}$ : trajectory of states, actions, and rewards
* $r_t^{\gamma}=\sum_{k=t}^{\infty} \gamma^{k-t}r(s_k, a_k)$ where $0 < \gamma < 1$ : return, total discounted reward from time-step $t$
* Value functions are defined to be the expected total discounted reward
  * $V^{\pi}(s) = \mathbb{E}[r_1^{\gamma} \vert S_1 = s; \pi]$
  * $Q^{\pi}(s,a)=\mathbb{E}[r_1^{\gamma} \vert S_1 = s, A_1 = a; \pi]$
* $J(\pi) = \mathbb{E} \left[ r_1^{\gamma} \vert \pi \right]$ : performance objective
* $p(s \rightarrow s', t, \pi)$ : density at state $s'$ after transitioning for $t$ time steps from state $s$
* $\rho^\pi(s') := \int_{\mathcal{S}} \sum_{t=1}^{\infty} \gamma^{t-1} p_1(s) p(s \rightarrow s', t, \pi) \text{d}s$ : (improper) discounted state distribution

$$
\begin{align*}
    J(\pi_{\theta}) & = \int_{\mathcal{S}} \rho^{\pi}(s) \int_{\mathcal{A}} \pi_{\theta}(s,a) r(s,a) \text{d}a \text{d}s \\ 
    & = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} [r(s,a)]
\end{align*}\tag{1}
$$

* $\mathbb{E}_{s \sim \rho}[ \cdot ]$ : (improper) expected value with respect to discounted state distribution $\rho(s)$

### 2.2. Stochastic Policy Gradient Theorem

$$
\begin{align*}
    \nabla_{\theta} J(\pi_{\theta}) & = \int_{\mathcal{S}} \rho^{\pi}(s) \int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a|s)Q^{\pi}(s,a) \text{d}a \text{d}s \\
    & = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s)Q^{\pi}(s,a) \right]
\end{align*} \tag{2}
$$

### 2.3. Stochastic Actor-Critic Algorithms

In general, substituting a function approximator $Q^w(s, a)$ for the true action-value function $Q^Ï€(s, a)$ may introduce bias. However, if the function approximator is compatible such that

* i) $Q^w(s,a) = \nabla_{\theta} \log \pi_{\theta}(a|s)^\top w$
  * compatible function approximators are linear in "features" of the stochastic policy, $\nabla_{\theta} \log \pi_{\theta}(a|s)$
* ii) the parameters $w$ are chosen to minimise the mean-squared error $\epsilon^2(w) = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ (Q^w(s, a) - Q^{\pi}(s, a))^2 \right]$
  * requires that the parameters are the solution to the linear regression problem that estimates $Q^{\pi}(s,a)$ from these features.
  * In practice, usually relaxed in favor of policy evaluation algorithms that estimate the value function more efficiently by temporal-difference learning;
there is no bias

$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) Q^w(s, a) \right] \tag{3}
$$

If both i) and i)) are satisfied then the overall algorithm is equivalent to not using a critic at all much like the REINFORCE algorithm.

### 2.4. Off-Policy Actor-Critic

