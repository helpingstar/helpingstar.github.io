---
title: "DQN, 심층 Q 네트워크"
date: 2022-09-27 18:51:26
lastmod : 2022-09-28 17:16:30
categories: RL
tag: [RL, DQN]
toc: true
toc_sticky: true
use_math: true
---

`DQN`은 $Q$함수를 근사하는 가치 기반 시간차(value-based temporal difference (TD)) 알고리즘이다. 학습된 $Q$ 함수를 이용하여 행동을 선택한다. `DQN`은 이산적 행동 공간을 갖는 환경에만 적용할 수 있다. `SARSA`와 달리 최적 $Q$ 함수를 학습함으로써 학습의 안정성과 속도를 향상시킨다. 즉 `DQN`은 `off-policy` 알고리즘이다. 최적 $Q$ 함수는 데이터를 수집하는 정책에 영향을 받지 않는다. 이론상 어떤 정책을 활용하여 데이터를 생성해도 상관 없으나 실제로는 더 적합한 정책이 존재한다.

`DQN`은 이전 경험과의 관계를 끊고 방대한 경험 재현 메모리로부터 일련의 경험을 무작위로 추출함으로써 경험을 재사용할 수 있다. 또한 동일한 경험을 사용하여 다수 파라미터의 업데이트를 수행할 수도 있다.

# `DQN`의 $Q$ 함수 학습

$$ Q^{\pi}_{\text{DQN}}(s,a) \approx r + \gamma\underset{a'}{\max}Q^{\pi}(s',a') \tag{4.1}$$
$$ Q^{\pi}_{\text{SARSA}}(s,a) \approx r + \gamma Q^{\pi}(s',a') \tag{4.2}$$
$$ Q^{\pi}_{\text{tar} \colon \text{DQN}}(s,a) = r + \gamma\underset{a'}{\max}Q^{\pi}(s',a') \tag{4.3}$$
$$ Q^{\pi}_{\text{tar}\colon\text{SARSA}}(s,a) = r + \gamma Q^{\pi}(s',a') \tag{4.4}$$

* $(4.1)$ : `DQN`의 벨만 방정식
* $(4.2)$ : `SARSA`의 벨만 방정식
* $(4.3)$ : `DQN`에서의 $Q^{\pi}_{\text{tar}}(s,a)$
* $(4.4)$ : `SARSA`에서의 $Q^{\pi}_{\text{tar}}(s,a)$

다음 상태 $s'$에서 실제로 취해진 행동 $a'$을 이용하여 $Q^{\pi}_{\text{tar}}(s,a)$를 추정하는 대신, `DQN`은 다음 상태에서 선택할 수 있는 잠재적인 모든 행동에 대한 $Q$ 가치 중에서 최대의 $Q$ 가치를 이용한다.

$Q^{\pi}_{\text{tar} \colon \text{DQN}}(s,a)$가 경험을 수집할 때 사용된 정책에 영향을 받지 않고 서튼과 바르토가 정의한 최적 정책의 영향을 받는다.

> 모든 상태에 대해 정책 $\pi'$의 기대 이득보다 크거나 같을 경우 정책 $\pi'$이 정책 $\pi$보다 더 좋거나 같은 수준이라고 정의된다. 다시 말해, 모든 $s \in \mathcal{S}$에 대해 $V^{\pi'}(s) \ge V^{\pi}(s)$일 경우에만 $\pi' \ge \pi$이다. 다른 모든 정책보다 좋거나 같은 수준인 정책은 언제나 적어도 하나 이상 존재한다. 이것이 최적 정책 $\pi^\ast$이다

최적 $Q$ 함수는 상태 $s$에서 행동 $a$를 선택하고 그 이후에는 최적 정책 $\pi^\ast$를 따르는 것으로서 정의된다. 아래에 표현되어 있다.

$$Q^\ast(s,a) = \underset{\pi}{\max}Q^\pi(s,a)=Q^{\pi^\ast}(s,a) \tag{4.5}$$

${Q^{\pi}_{\text{tar:DQN}}(s,a)}$를 다시 생각해보자.

$Q^\pi$의 추정값이 정확하다면 $Q^\pi(s',a')$을 최대화하는 행동을 선택하는 것이 최적일 것이다. 이것이 에이전트가 할 수 있는 최선이다. 이는 $Q^{\pi}_{\text{tar} \colon \text{DQN}}(s,a)$에 해당하는 정책이 최적 정책 $\pi^\ast$라는 것을 나타낸다.

# `DQN`의 행동 선택

`DQN` 에이전트가 경험울 수집하는 방법은 여전히 중요한데 이떄 고려해야 할 두가지 요소가 있다.

1. 탐험-활용 균형 문제를 마주한다. 훈련 초기에는 공간을 빠르게 탐험하고 훈련이 진행될 수록 탐험의 비율을 낮춰 이미 학습한 것을 활용하는데 더 많은 시간을 들여야 한다.
2. 상태-행동 공간이 연속적인 값으로 채워진 아주 큰 공간이거나 높은 차원의 이산적 공간이라면 모든 $(s,a)$ 쌍을 경험하기는 매우 어렵기 때문에, 경험하지 못한 $(s,a)$ 쌍은 $Q$ 가치가 정확하지 않을 수 있다.

---

**일반화와 신경망**

표 형태로 함수를 표현하면 각기 다른 상태와 행동이 서로 어떻게 연관되는지에 대해 어떤 것도 학습할 수 없다.

반면에 신경망은 이미 경험한 $(s,a)$에 대한 $Q$ 가치로부터 경험하지 못한 $(s',a')$을 예측 할 수 있다. 신경망이 각기 다른 상태와 행동이 서로 어떻게 연관되는지 학습하기 때문이다.

하지만 신경망도 일반화 능력에는 한계가 있다.
1. 신경망에 들어오는 입력이 신경망을 훈련할 때 사용했던 입력과 상당히 다를 경우 신경망이 올바른 값을 출력할 확률이 낮다. 일반적으로, 훈련 데이터를 둘러싼 좁은 영역으로부터 입력이 들어올 때 일반화 능력이 훨씬 더 좋다.
2. 신경망이 근사하려고 하는 함수가 급격한 불연속점을 갖는다면 신경망을 통한 일반화가 잘되지 않을 가능성이 높다. 이것은 신경망이 암묵적으로 가정하는 입력 공간이 특정 영역 내에서 부드럽고 연속적인 입력 공간이기 때문이다. $x$와 $x'$이 비슷하다면 출력 $y$와 $y'$도 비슷해야 한다.

---

상태-행동 공간이 크다면 좋은 정책이 자주 경험할 것 같은 상태와 행동에 집중하여 훈련한다면 이러한 환경에서도 좋은 성능을 낼 수 있다.

따라서 `DQN` 에이전트의 정책은 현재 $Q$ 함수 추정값에 대해 탐욕적으로 행동함으로써 경험하게 될 상태-행동과 상당히 유사한 상태-행동을 경험하도록 해야 한다. 이렇게 하면 에이전트의 현재 $Q$ 함수 추정값이 최적 정책의 $Q$ 함수 추정값이 된다. 이 두 정책이 생성하는 데이터의 분포는 유사해야 한다.

실제로 `ε-greedy policy` 또는 `Boltzmann policy` 정책을 이용하면 에이전트가 최적 정책의 추정 값을 사용하여 경험할 법한 데이터에 집중해서 학습하도록 도와준다.

`Boltzmann policy`에 대한 내용은 본 블로그의 포스트 [[볼츠만 머신, Boltzmann machine]](https://helpingstar.github.io/dl/other_network/) 에서도 확인할 수 있다.


> 출처
 - Laura Graesser, Wah Loon Keng,『단단한 심층 강화학습』, 김성우, 제이펍(2022)
