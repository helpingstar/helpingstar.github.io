---
layout: single
title: "[ch09] 근사를 이용한 활성 정책 예측"
date: 2022-09-02 16:24:27
lastmod : 2022-09-02 16:24:25
categories: RL
tag: [Sutton, 단단한 강화학습]
toc: true
toc_sticky: true
use_math: true
---

**활성 정책**으로부터 **상태가치 함수**를 추정하는 과정을 서술한다

* chap09 : 활성, 가치함수 근사
* chap10 : 활성, 최적정책 근사
* chap11 : 비활성, 정책 근사 (어렵다고 함)
* chap12 : eligibility trace

후에는 다음 내용들이 서술된다.

전에는 상태함수가 표 형태로 표현되었으나 이제는 가중치 벡터 $\textbf{w}\in\mathbb{R}^{d}$ 파라미터화된 함수 형태를 갖는다

$\hat{v}(s, \textbf{w}) \approx v_{\pi}(s)$ : 가중치

$\textbf{w}$가 굵은 글씨인 이유는 단일 숫자가 아닌 여러 숫자로 이루어진 벡터이기 때문인 듯 하다.

일반적으로, $\hat{v}$는 다층 인공 신경망에 의해 계산된 함수이고, $\textbf{w}$는 모든 층의 연결된 가중치 벡터일 수도 있다.

일반적으로 가중치의 개수($\textbf{w}$의 차원)은 상태의 개수보다 훨씬 적다($d \ll \mid \mathcal{S} \mid$), 결과적으로 하나의 상태가 갱신되면 그 변화는 다른 상태의 가치에 영향을 준다. 이러한 **일반화**(generalization)가 학습을 강력하게도, 이해가 어렵게도 만든다.

아래 그림을 보면 하나의 갱신이 다른 상태 가치에 영향을 준다는 말이 감이 올 것이다.

![Two_layer_ann](../../assets/images/ai/Two_layer_ann.svg){: width="80%" height="80%" class="align-center"}

# 가치 함수 근사

이책의 모든 예측 방법은 특정 상태에서의 가치를 그 상태에 대한 '보강된 가치' 또는 **갱신 목표**(update target)를 향해 이동시키기 위한 가치 함수 추정값의 갱신으로 설명된다.

$s \mapsto u$
* $s$ : 상태(갱신의 대상)
* $u$ : $s$의 가치 추정값이 도달해야 하는 갱신의 목표

**ex)**

* `Monte Carlo` : $S_t \mapsto G_t$
* `TD(0)` : $S_t \mapsto R_{t+1}+\gamma \hat{v}(S_{t+1}, \textbf{w}_t)$
* `n-step TD` : $S_t \mapsto G_{t:t+n}$
* `DP policy-evaluation` : $s \mapsto \mathbb{E}_{\pi} [R_{t+1} + \gamma \hat{v}(S_{t+1}, \textbf{w}_{t}) \vert S_{t}=s]$

입출력 예제를 모사하기 위해 이러한 방식으로 학습하는 기계학습 방법을 **지도학습**(supervised learning) 방법이라 하고, 출력이 $u$와 같은 숫자일 경우 이 과정은 **함수 근사**(function approximation)라고도 한다. 함수 근사 방법은 근사하려고 하는 함수의 바람직한 입출력 관계를 나타내는 예제가 주어진다는 것을 전제로 한다.

강화학습에서 중요한 점
* 학습자가 환경이나 환경의 모델과 상호작용하며 온라인으로 학습할 수 있는 능력
  * 점증적으로 획득되는 데이터로부터 효율적으로 학습할 수 있는 방법 필요
* 일반적으로 nonstationary 목표 함수를 다룰수 있는 함수 근사 방법을 필요로 한다.
  * ex) 정챍이 변하지 않았다 해도, 훈련 예제의 target value가 bootstrapping method(DP, TD)로 생성되었다고 하면 그것은 nonstationary이다.
  * Methods that cannot easily handle such nonstationarity are less suitable for reinforcement learning.

# The prediction objective

Tabular Case
* 학습된 가치 함수가 실제 가치 함수와 정확하게 같아질 수 있기 때문에(수렴한다는 뜻인 것 같다) 예측의 품질을 연속적으로 측정할 필요가 없다.
* 한 상태의 갱신이 다른 상태에 영향을 주지 않는다.

함수 근사
* 한 상태의 갱신이 다른 상태에 영향을 준다.
* 상태보다 가중치가 적다는 가정 하에 한 상태의 가치 추정값을 더 정확하게 만드는 것은 언제나 다른 상태를 덜 정확하게 만드는 것을 의미한다.

따라서 각 상태 $s$에 대해 가치 추정의 오차를 줄이는 노력을 얼마나 기울일 것인가를 나타내는 상태 분포 $\mu(s) \geq 0, \sum_a \mu(s)=1$을 명시해야 한다.

(노력의 정도는 0 이상이어야 하고 각 상태에 1을 분배한다)

상태 공간에 걸쳐 $\mu$를 할당함으로써 자연스러운 목적 함수를 얻는다

$\overline{VE}(\textbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) [v_{\pi}-\hat{v}(s,\textbf{w})]^2$

근사 가치가 실제 가치와 얼마나 차이를 갖는지에 대한 지표

대체로 $\mu(s)$ 는 상태 $s$에서 소비된 시간의 비율이 되도록 선택된다.

활성 정책 훈련의 경우 이것은 **활성 정책 분포**라고 불린다.
연속적인 문제에서 활성 정책 분포는 정책 $\pi$를 따르는 stationary 분포다.

$\overline{VE}$ 의 측면에서 global optimum을 찾으려고 하겠지만 복잡한 함수를 근사할 경우 local optimum으로 수렴하는 것을 찾으려고 할 수도 있다. 이는 global optimum을 보장하지 않지만 비선형 함수 근사에서 최선이고 충분한 경우가 꽤 있다.

많은 강화학습 문제에서 최적값 또는 근처의 제한되는 영역으로라도 값이 수렴한다는 것을 보장하지 못하며 어쩌면 $\overline{VE}$가 발산할 수도 있다.

# Stochastic-gradient and Semi-gradient Methods

**확률론적 경사도 강하**방법은 각 예제에 대해 오차를 가장 많이 감소시키는 방향으로 가중치 벡터를 조금씩 조정함으로써 관측된 예제의 오차를 최소화한다.

$\textbf{w}_{t+1} \doteq \textbf{w}_t - \frac{1}{2} \alpha \nabla [v_{\pi}(S_t) - \hat{v}(S_t, \textbf{w}_t)]^2$

$=\textbf{w}_t - \alpha[v_{\pi}(S_t) - \hat{v}(S_t, \textbf{w}_t)]\nabla \hat{v}(S_t,\textbf{w}_t)$

* $\alpha$ : 양의 시간 간격 파라미터

$\nabla f(\textbf{w}) \doteq (\frac{\partial f(\textbf{w})}{\partial w_1},\frac{\partial f(\textbf{w})}{\partial w_2},\cdots,\frac{\partial f(\textbf{w})}{\partial w_d})^T$

$\nabla f(\textbf{w})$ : 함수 $f(\textbf{w})$를 벡터의 성분에 대해 편미분한 열 벡터

**경사도 방향으로 작은 간격만을 취하는 이유**

모든 상태에 대해 오차가 0이 되는 가치 함수를 찾는 것이 아니라, 서로 다른 상태들 사이의 오차에 대해 균형을 맞추는 근사만을 추구하거나 기대한다.

---

$\textbf{w}_{t+1}=\textbf{w}_t - \alpha[U_t - \hat{v}(S_t, \textbf{w}_t)]\nabla \hat{v}(S_t,\textbf{w}_t)$

* $U_t$ : 실제 가치의 근삿값, $v_{\pi}(S_t)$를 $U_t$로 대체함으로써 $v_{\pi}(S_t)$를 근사한다

**몬테카를로**

목표인 $U_t \doteq G_t$는 그 정의상 $v_{\pi}(S_t)$의 편차 없는 추정값이 되므로 locally optimal approximation으로 수렴한다.(=locally optimal solution을 찾는 것이 보장)

![gradient_monte_carlo_algorithm](../../assets/images/rl/9_3_gradient_monte_carlo_algorithm.png){: width="80%" height="80%" class="align-center"}

**부트스트랩**

$v_{\pi}(S_t)$에 대한 부트스트랩 추정값이 목표 $U_t$로서 사용된다면 이와 동일한 보장(locally optimal approximation으로 수렴)을 받지 못한다.

`n-step returns` $G_{t:t+n}$이나 `DP target` $\sum_{a,s',r} \pi(a \mid S_t)p(s',r \mid S_t, a)[r + \gamma \hat{v}(s', \textbf{w}_t)]$ 는 모두 가중치 벡터 $\textbf{w}_t$의 영향을 받는데, 이것은 부트스트랩 목표가 편차를 갖게 되어 진정한 경사도 강하 방법을 형성하지 못할것을 암시함

한가지 관점은

$\textbf{w}_{t+1} \doteq \textbf{w}_t - \frac{1}{2} \alpha \nabla [v_{\pi}(S_t) - \hat{v}(S_t, \textbf{w}_t)]^2$

$=\textbf{w}_t - \alpha[v_{\pi}(S_t) - \hat{v}(S_t, \textbf{w}_t)]\nabla \hat{v}(S_t,\textbf{w}_t)$

의 핵심 단계가 $\textbf{w}_t$와는 독립적으로 존재하는 목표에 영향을 받는다고 보는 것이다

($\textbf{w}_t$가 바뀌면 bootstrapping에서 미래의 예측치인 target도 변하게 되는데 이는 현재 예측치와 독립적이지 않다.)

부트스트랩 방법은 가중치 벡터 $\textbf{w}_t$를 변경하는 것이 추정값에 미치는 효과를 고려하지만 target에 미치는 효과는 무시한다. 이 방법은 오직 경사도의 일부만을 포함하고 따라서 이 방법을 **준경사도 방법**(semi-gradient method)이라고 부른다.

준경사도 방법은 경사도 방법처럼 안정적으로 수렴하지는 않지만, 선형 근사 함수의 경우와 같은 중요한 경우에 있어서는 안정적으로 수렴한다.

일반적으로 빠른 학습을 가능하게 하는 장점이 있고 에피소드가 끝날때까지 기다릴 필요가 없다(bootstrapping의 장점)

![gradient_monte_carlo_algorithm](../../assets/images/rl/9_3_semi_gradient_TD.png){: width="80%" height="80%" class="align-center"}

> 출처
 - Richard S. Sutton,『단단한 강화학습』, 김성우, 제이펍(2020)
 - https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Two_layer_ann.svg

$s \mapsto u$

$v_{\pi}(S_t)$