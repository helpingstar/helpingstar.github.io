---
layout: single
title: "TRPO, Trust Region Policy Optimization"
date: 2023-10-29 15:43:54
lastmod: 2023-10-29 15:43:52
categories: RL
tag: [RL, TRPO]
toc: true
toc_sticky: true
use_math: true
---

(수정중... 지속적으로 수정될 예정)

$\eta(\pi)$ : expected discounted reward.

$$\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0, a_0, \cdots, \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t}A_{\pi}(s_t, a_t) \right ] \tag{1}$$
* *Kakade & Langford (2002)*
* $s_0, a_0, \cdots,=\tau$


$$
\begin{aligned}
    \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t}A_{\pi} (s_t, a_t) \right ] & = \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t} (r(s_t) + \gamma V_{\pi}(s_{t+1})-V_{\pi}(s_t)) \right ] \\
    & = \eta(\tilde{\pi}) + \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t+1} V_{\pi}(s_{t+1}) - \sum_{t=0}^{\infty}\gamma^{t}V_{\pi}(s_t) \right ] \\
    & = \eta(\tilde{\pi}) - \mathbb{E}_{s_0} \left [ V_{\pi} (s_0) \right ] \\
    & = \eta(\tilde{\pi}) - \eta(\pi)
\end{aligned}
$$

$$
\rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \cdots
$$


$$
\begin{align}
\eta(\tilde{\pi}) &= \eta(\pi) + \sum_{t=0}^{\infty} \sum_s P(s_t = s|\tilde{\pi}) \sum_a \tilde{\pi}(a|s)\gamma^t A_\pi(s,a) \notag \\
&= \eta(\pi) + \sum_{t=0}^{\infty} \sum_s \gamma^t P(s_t = s|\tilde{\pi}) \sum_a \tilde{\pi}(a|s)A_\pi(s,a) \notag \\
&= \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{2}
\end{align}
$$

$$\rho_{\tilde{\pi}} \rightarrow \rho_{\pi}$$

$$
L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{3}
$$

$$
\begin{align*}
A(s, a) & = Q(s, a) - V(s) \\
& = Q(s, a) - \mathbb{E}_a \left [ Q(s, a) \right ]
\end{align*}
$$
* $\mathbb{E}_a$ : action 에 대한 expectation

$$
\begin{align*}
\mathbb{E}_a \left [ A(s, a) \right ] & = Q(s, a) - V(s) \\
& = \mathbb{E}_a \left [ Q(s, a) - \mathbb{E}_a \left [ Q(s, a) \right ]\right ] \\
& = 0 \quad \text{(same policy)}
\end{align*}
$$

* 같은 policy에 대해서 위 식은 0과 같다.

$$
\begin{align*}
L_{\pi_{\theta_0}} (\pi_{\theta_0}) &= \eta(\pi_{\theta_0}),\\
\nabla_{\theta} L_{\pi_{\theta_0}} (\pi_{\theta})\big|_{\theta=\theta_0} &= \nabla_{\theta} \eta(\pi_{\theta})\big|_{\theta=\theta_0}. \tag{4}
\end{align*}
$$

$$
\because L_{\pi_{\theta_0}}(\pi_{\theta_0}) = \eta(\pi_{\theta_0}) + \sum_s \rho_{\pi_{\theta_0}} \sum_a \pi_{\theta_0}(a|s)A_{\pi_{\theta_0}}(s,a).
$$
* *Kakade & Langford (2002)*

---

$$\pi_\text{new}(a|s)=(1 - \alpha)\pi_{\text{old}}(a|s) + \alpha \pi' (a|s) \tag{5}$$
* *Kakade & Langford (2002)*
* $\pi_\text{old}$ : current policy
* $\pi'=\arg \max_{\pi'}L_{\pi_\text{old}}(\pi')$
* $\alpha$ 비율 만큼만 new policy

$$
\begin{align*}
\eta(\pi_{\text{new}}) & \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{2\epsilon\gamma}{(1-\gamma)^2} \alpha^2, \\
& \text{where } \epsilon = \max_s \left| \mathbb{E}_{a\sim\pi'}(a|s) [A_{\pi}(s,a)] \right|. \tag{6}
\end{align*}
$$

* *Kakade & Langford (2002)*
* $\alpha$ ↑ ⇒ 더 많은 new policy ⇒ 오차 ↑ ⇒ lower bound ↓
* $\gamma$ ↑ ⇒ time step이 커질수록 discount 양 ↓ ⇒ lower bound ↓

Our principal theoretical result is that the policy improvement bound in Equation (6) can be extended to general stochastic policies, rather than just mixture polices, by replacing $α$ with a distance measure between $π$ and $\tilde{π}$, and changing the constant $ϵ$ appropriately.

우리의 주요 이론적 결과는 식(6)의 policy improvement bound가 mixture policy뿐만 아니라 일반 확률적 정책으로 확장될 수 있음을 보여준다. $α$를 $π$와 $\tilde{π}$ 사이의 거리 측정으로 대체하고, 상수 $ϵ$를 적절히 변경함으로써 가능하다.

**Total Variation Divergence**

$$
D_{\text{TV}}(p \parallel q) = \frac{1}{2} \sum_i |p_i - q_i|
$$

$$
D^{\text{max}}_{\text{TV}}(\pi, \tilde{\pi}) = \max_s D_{TV}(\pi(\cdot|s) \parallel \tilde{\pi}(\cdot|s)). \quad (7)
$$

**Theorem 1.** $Let \: \alpha = D_{\text{TV}}^{\text{max}}(\pi_{\text{old}}, \pi_{\text{new}})$, *Then the following bound holds:*

$$
\begin{align*}
\eta(\pi_{\text{new}}) &\geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} \alpha^2 \\
\text{where } & \epsilon = \max_{s,a} |A_{\pi}(s, a)| \tag{8}
\end{align*}
$$

$$
D_{TV}(p \| q)^2 \leq D_{KL}(p \| q)
$$

* *Pollard (2000, Ch. 3)*

$$
D^{\text{max}}_{\text{KL}}(\pi, \tilde{\pi}) = \max_s D_{KL}(\pi(\cdot|s) \| \tilde{\pi}(\cdot|s))
$$

$$
\begin{align*}
\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) & - C D^{\text{max}}_{KL}(\pi, \tilde{\pi}), \\ \text{where } C & = \frac{4\epsilon\gamma}{(1 - \gamma)^2}. \tag{9}
\end{align*}
$$

**Theorem 1.** 정리

$$
\begin{align*}
\eta(\tilde{\pi})& \geq L_{\pi}(\tilde{\pi}) - \frac{2\epsilon\gamma}{(1-\gamma)^2} \alpha^2,
\quad \text{where } \epsilon = \max_s \left| \mathbb{E}_{a\sim\pi'}(a|s) [A_{\pi}(s,a)] \right|.\\

& \geq L_{\pi}(\tilde{\pi}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} \alpha^2, \quad \text{where } \epsilon = \max_{s,a} |A_{\pi}(s, a)|, \: \alpha = D_{\text{TV}}^{\text{max}}(\pi, \tilde{\pi}) \\

& \geq L_{\pi}(\tilde{\pi}) - C D^{\text{max}}_{KL}(\pi, \tilde{\pi}), \quad 
\text{where } C = \frac{4\epsilon\gamma}{(1 - \gamma)^2}.
\quad (\because D_{TV}(p \| q)^2 \leq D_{KL}(p \| q))
\end{align*}
$$

**Minorization-Maximization(MM) Algorithm** - *(Hunter & Lange, 2004)*

$$
M_i(\pi) = L_{\pi_i}(\pi) - C D^{\text{max}}_{KL}(\pi_i, \pi)
$$

$$
\begin{align*}
& \eta(\pi_{i+1}) \geq M_i(\pi_{i+1}) \quad (\because \eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) - C D^{\text{max}}_{KL}(\pi, \tilde{\pi})) \\
& \eta(\pi_i) = M_i(\pi_i), \text{ therefore, } \\
& \eta(\pi_{i+1}) - \eta(\pi_i) \geq M_i(\pi_{i+1}) - M(\pi_i). \tag{10}
\end{align*}
$$

다르게 표현

$$
\begin{align*}
\eta(\pi_i) & = M_i(\pi_i) \\
& \leq M_i(\pi_{i+1}) \quad (\because \pi_{i+1}=\underset{\pi}{\arg\max}M_i(\pi))\\
& \leq \eta(\pi_{i+1}) \quad (\because \eta(\pi) \geq M_i(\pi))
\end{align*}
$$

## Optimization of Parameterized Policies

Since we consider parameterized policies $\pi_{\theta}(a|s)$ with parameter vector $\theta$, we will overload our previous notation to use functions of $\theta$ rather than $\pi$.
* $\eta(\theta) \coloneqq \eta(\pi_\theta)$
* $L_{\theta}(\tilde{\theta}) \coloneqq L_{\pi_{\theta}}(\pi_{\tilde{\theta}})$
* $D_{\text{KL}}(\theta \: || \: \tilde{\theta}) \coloneqq D_{\text{KL}}(\pi_{\theta} \| \pi_{\bar{\theta}})$
* $\theta_{\text{old}}$ : previous policy paramters that we want to improve upon.

$$
\underset{\theta}{\text{maximize}} \left [ L_{\theta_{\text{old}}}(\theta) - C D_{\text{KL}}^{\text{max}}(\theta_{\text{old}}, \theta) \right ]
$$

$$
\begin{align*}
& \underset{\theta}{\text{maximize }}L_{\theta_{\text{old}}}(\theta) \tag{11}\\
& \text{subject to } D_{\text{KL}}^{\text{max}}(\theta_{\text{old}}, \theta) \leq \delta
\end{align*}
$$

$$
\overline{D}^{\rho}_{\text{KL}}(\theta_1, \theta_2) \coloneqq \mathbb{E}_{s \sim \rho} \left[ D_{KL}(\pi_{\theta_1}(\cdot | s) \: || \: \pi_{\theta_2}(\cdot | s)) \right].
$$

$$
\begin{align*}
& \underset{\theta}{\text{maximize }} L_{\theta_{\text{old}}}(\theta) \tag{12} \\
& \text{subject to} \; \overline{D}^{\rho_{\theta_{\text{old}}}}_{\text{KL}}(\theta_{\text{old}}, \theta) \leq \delta.
\end{align*}
$$

## Sample-Based Estimation of the Objective and Constraint

Expanding $L_{\theta_{\text{old}}}$ in equation $(12)$

$$
\begin{align*}
& \underset{\theta}{\text{maximize }} \sum_{s} \rho_{\theta_{\text{old}}}(s) \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a)
 \tag{13} \\
& \text{subject to} \; \overline{D}^{\rho_{\theta_{\text{old}}}}_{\text{KL}}(\theta_{\text{old}}, \theta) \leq \delta.
\end{align*}
$$

* Replace $\sum_{s}\rho_{\theta_\text{old}}(s) \left[ ... \right]$ in the objective by the expectation $\frac{1}{1 - \gamma} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ ... \right]$
  * $\rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \cdots$
  * state visited frequency가 확률의 모습을 가지려면 $(1-\gamma)$를 곱해준다. 
  * maximize시에 상수를 곱한 것은 의미가 없으므로 $\frac{1}{1-\gamma}$를 제거해도 된다.
* Replace the advantage values $A_{\theta_{\text{old}}}$ by the Q-values $Q_{\theta_\text{old}}$ in Equation $(13)$
  * $A(s, a) = Q(s, a) - V(s)$
  * $V(s)$ : policy에 대해 constant 이며 policy에 아무 관계가 없다.
  * maximize시에 사용하는 $\theta$는 변하지 않으므로 $A$ → $Q$ 가능하다.
* Replace the sum over the actions by an importance sampling estimator. 
  * New policy 사용 ⇒ Monte Carlo Sampling 불가 ⇒ Importance Sampling 사용하여 해결
    * New policy에 대한 sample이 없으므로 최대한 $\pi_\theta$를 $\pi_{\theta_{\text{old}}}$로 바꿔야 현재 current policy에서 주어진 샘플들을 사용할 수 있다.
  * Using $q$ to denote the sampling distribution, the contribution of a single $s_n$ to the loss function is
    * $\sum_a \pi_\theta(a|s_n) A_{\theta_{\text{old}}}(s_n, a) = \mathbb{E}_{a\sim q} \left[ \frac{\pi_\theta(a|s_n)}{q(a|s_n)} A_{\theta_{\text{old}}}(s_n, a) \right].$


$$
\begin{align*}
\sum_{s} \rho_{\theta_{\text{old}}}(s) \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a) & ⇒ \frac{1}{1-\gamma} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a) \right] \\

& ⇒ \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a) \right] \\

& ⇒ \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ \mathbb{E}_{a \sim q} \left[ \frac{\pi_{\theta}(a|s)}{q(a|s)} A_{\theta_{\text{old}}}(s,a) \right] \right] \\

& ⇒ \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim q} \left[ \frac{\pi_{\theta}(a|s)}{q(a|s)} Q_{\theta_{\text{old}}}(s,a) \right]
\end{align*}
$$

Our optimization problem in Equation $(13)$ is exactly equivalent to the following one, written in terms of expectations:

$$
\begin{align*}
\underset{\theta}{\text{maximize }} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim q} \left[ \frac{\pi_\theta(a|s)}{q(a|s)} Q_{\theta_{\text{old}}}(s, a) \right] \tag{14} \\
\text{subject to } \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ D_{\text{KL}}(\pi_{\theta_\text{old}}(\cdot|s) \: || \: \pi_\theta(\cdot|s)) \right] \leq \delta.
\end{align*}
$$

## Parctical Algorithm

$$
\frac{1}{N} \sum_{n=1}^{N} \frac{\partial^2}{\partial \theta_i \partial \theta_j} D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot|s_n) \: || \: \pi_{\theta}(\cdot|s_n)),
$$

$$
\frac{1}{N} \sum_{n=1}^{N} \frac{\partial}{\partial \theta_i} \log \pi_{\theta}(a_n|s_n) \frac{\partial}{\partial \theta_j}\log \pi_{\theta}(a_n|s_n).
$$

## Connections with Prior Work

$$
\begin{align*}
& \underset{\theta}{\text{maximize}}  \left[ \nabla_{\theta} L_{\theta_{\text{old}}}(\theta) |_{\theta=\theta_{\text{old}}} \cdot (\theta - \theta_{\text{old}}) \right] \tag{17} \\
& \text{subject to } \frac{1}{2} (\theta_{\text{old}} - \theta)^T A(\theta_{\text{old}}) (\theta_{\text{old}} - \theta) \leq \delta, \\
& \text{where } A(\theta_{\text{old}})_{ij} = \\ 
& \frac{\partial}{\partial \theta_i}\frac{\partial}{\partial \theta_j} \mathbb{E}_{s \sim \rho_{\pi}} \left[ D_{KL} \left( \pi( \cdot | s, \theta_{\text{old}}) \: || \: \pi( \cdot | s, \theta) \right) \right] |_{\theta=\theta_{\text{old}}}
\end{align*}
$$

$$
\theta_{\text{new}} = \theta_{\text{old}} + \frac{1}{\lambda} A(\theta_{\text{old}})^{-1} \left. \nabla_{\theta} L(\theta) \right|_{\theta=\theta_{\text{old}}}
$$

We can also obatin the standard policy gradient update by using an $\ell_2$ constraint or penalty:

$$
\begin{aligned}
&\underset{\theta}{\text{maximize}} & & \left[ \left. \nabla_{\theta} L_{\text{old}}(\theta) \right|_{\theta=\theta_{\text{old}}} \cdot (\theta - \theta_{\text{old}}) \right] \\
&\text{subject to} & & \frac{1}{2} \|\theta - \theta_{\text{old}}\|^2 \leq \delta.
\end{aligned}
$$

