---
layout: single
title: "(WIP)TRPO, Trust Region Policy Optimization"
date: 2023-10-29 15:43:54
lastmod: 2023-10-29 15:43:52
categories: RL
tag: [RL, TRPO]
toc: true
toc_sticky: true
use_math: true
published: false
---

(수정중... 지속적으로 수정될 예정)

* 논문 : [https://arxiv.org/abs/1502.05477](https://arxiv.org/abs/1502.05477)
* 참고자료
  * [오승상 강화학습 TRPO 1/2/3](https://youtu.be/c15b9AjHxBA?si=o0SuPynSb0z3cJgE)

## Preliminaries

$\eta(\pi)$ : expected discounted reward. **(최대화 하기를 원하는 것)**

$$\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0, a_0, \cdots, \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t}A_{\pi}(s_t, a_t) \right ] \tag{1}$$

* *Kakade & Langford (2002)*
* $s_0, a_0, \cdots,=\tau$

$$
\begin{aligned}
    \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t}A_{\pi} (s_t, a_t) \right ] & = \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t} (r(s_t) + \gamma V_{\pi}(s_{t+1})-V_{\pi}(s_t)) \right ] \\
    & = \eta(\tilde{\pi}) + \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t+1} V_{\pi}(s_{t+1}) - \sum_{t=0}^{\infty}\gamma^{t}V_{\pi}(s_t) \right ] \\
    & = \eta(\tilde{\pi}) - \mathbb{E}_{s_0} \left [ V_{\pi} (s_0) \right ] \\
    & = \eta(\tilde{\pi}) - \eta(\pi)
\end{aligned}
$$

---

$\rho_{\pi}$ be the (unnormalized) discounted visitation frequencies.

$$
\begin{align*}
\rho_\pi(s) & = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \cdots \\
& = \sum_{t=0}^{\infty} \gamma^{t}P(s_t=s)
\end{align*}
$$

---

$$
\begin{align*}
E_{\tau \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_{\pi}(s_t, a_t) \right] & = \sum_{t=0}^{\infty} P(s_t = s | \tilde{\pi}) \sum_a \tilde{\pi}(a | s) \gamma^t A_{\pi}(s, a) \\
& = \sum_s \sum_{t=0}^{\infty} \gamma^t P(s_t = s | \tilde{\pi}) \sum_a \tilde{\pi}(a | s) A_{\pi} (s, a) \\
& = \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a | s) A_{\pi} (s, a)
\end{align*}
$$

접근법

아래 두개는 더해주는 순서만 다르다.

* $E_{\tau \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_{\pi}(s_t, a_t) \right]$
  * 모든 timestep에 대해 advantage function을 더함
  * 에피소드가 일어날 확률을 곱해 expectation을 구함

![trpo_0_1](../../assets/images/rl/paper_review/trpo/trpo_0_1.png){: width="50%" height="50%"}

* $\sum_{t=0}^{\infty} P(s_t = s \mid \tilde{\pi}) \sum_a \tilde{\pi}(a \mid s) \gamma^t A_{\pi}(s, a)$
  * timestep 고정
    * 주어진 timestep에서 해당 state가 나올 확률들을 가지고 expectation을 구함 $P(s_t=s \mid \tilde{\pi})$
    * state에서 해당 action이 나올 확률을 가지고 expectation을 구함 $\tilde{\pi}(a \mid s)$
  * timestep을 정한 후에 advantage function의 expectation을 구함
    * 모든 timestep에 대해 더해준다.

![trpo_0_2](../../assets/images/rl/paper_review/trpo/trpo_0_2.png){: width="50%" height="50%"}

---

$$
\begin{align}
\eta(\tilde{\pi}) &= \eta(\pi) + \sum_{t=0}^{\infty} \sum_s P(s_t = s|\tilde{\pi}) \sum_a \tilde{\pi}(a|s)\gamma^t A_\pi(s,a) \notag \\
&= \eta(\pi) + \sum_{t=0}^{\infty} \sum_s \gamma^t P(s_t = s|\tilde{\pi}) \sum_a \tilde{\pi}(a|s)A_\pi(s,a) \notag \\
&= \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{2}
\end{align}
$$

> However, in the approximate setting, it will typically be unavoidable, due to estimation and approximation error, that there will be some states s for which the expected advantage is negative, that is, $\sum_a \tilde{\pi}(a \mid s)A_\pi(s,a) < 0$.

* 목표 : $\eta(\tilde{\pi}) \geq \eta(\pi)$
  * 조건 : $\sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a \mid s)A_\pi(s,a) \geq 0$
  * $\rho_{\tilde{\pi}}$가 확률이기 때문에 모든 state에 대해 $\sum_a \tilde{\pi}(a \mid s)A_\pi(s,a) \geq 0$ 이어야 한다.

> The complex dependency of $\rho_{\tilde{\pi}}$ on $\tilde{\pi}$ makes Equation (2) difficult to optimize directly

* 문제 : 찾아야 하는 new policy에 대한 state visitation frequency $\rho_{\tilde{\pi}}$는 구하기 힘들다.
  * 근사 : $\rho_{\tilde{\pi}} \rightarrow \rho_{\pi}$

$$
L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{3}
$$

$$
\begin{align*}
A(s, a) & = Q(s, a) - V(s) \\
& = Q(s, a) - \mathbb{E}_a \left [ Q(s, a) \right ]
\end{align*}
$$

* $\mathbb{E}_a$ : action 에 대한 expectation

$$
\begin{align*}
\mathbb{E}_a \left [ A(s, a) \right ] & = Q(s, a) - V(s) \\
& = \mathbb{E}_a \left [ Q(s, a) - \mathbb{E}_a \left [ Q(s, a) \right ]\right ] \\
& = 0 \quad \text{(same policy)}
\end{align*}
$$

* Advantage function에서 같은 policy로 모든 action에 expectation을 취하면 0이 된다.

$$
\begin{align*}
L_{\pi_{\theta_0}} (\pi_{\theta_0}) &= \eta(\pi_{\theta_0}), \quad (\because \sum_s \rho_{\pi_{\theta_0}} \sum_a \pi_{\theta_0}(a|s)A_{\pi_{\theta_0}}(s,a) = 0) \\
\nabla_{\theta} L_{\pi_{\theta_0}} (\pi_{\theta})\big|_{\theta=\theta_0} &= \nabla_{\theta} \eta(\pi_{\theta})\big|_{\theta=\theta_0}. \tag{4}
\end{align*}
$$

* *Kakade & Langford (2002)*

---

$$\pi_\text{new}(a|s)=(1 - \alpha)\pi_{\text{old}}(a|s) + \alpha \pi' (a|s) \tag{5}$$

* *Kakade & Langford (2002)*
* $\pi_\text{old}$ : current policy
* $\pi'=\arg \max_{\pi'}L_{\pi_\text{old}}(\pi')$
* $\alpha$ 비율 만큼만 new policy

$$
\begin{align*}
\eta(\pi_{\text{new}}) & \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{2\epsilon\gamma}{(1-\gamma)^2} \alpha^2, \\
& \text{where } \epsilon = \max_s \left| \mathbb{E}_{a\sim\pi'}(a|s) [A_{\pi}(s,a)] \right|. \tag{6}
\end{align*}
$$

* *Kakade & Langford (2002)*
* $\alpha$ ↑ ⇒ 더 많은 new policy ⇒ 오차 ↑ ⇒ lower bound ↓
* $\gamma$ ↑ ⇒ time step이 커질수록 discount 양 ↓ ⇒ lower bound ↓
  * $\gamma$ ↑ ⇒ 분모가 작아짐, 분자가 커짐 ⇒ 음의 방향이므로 lower bound가 작아짐
* $\epsilon$
  * 모든 state에 대해 max 값을 구하는 것이 어려움
  * new policy $\tilde{\pi}$를 사용하는 것이 힘듬

## Monotonic Improvement Guarantee for General Stochastic Policies

> Our principal theoretical result is that the policy improvement bound in Equation (6) can be extended to general stochastic policies, rather than just mixture polices, by replacing $α$ with a distance measure between $π$ and $\tilde{π}$, and changing the constant $ϵ$ appropriately. <br><br> 우리의 주요 이론적 결과는 식(6)의 policy improvement bound가 mixture policy뿐만 아니라 일반 확률적 정책으로 확장될 수 있음을 보여준다. $α$를 $π$와 $\tilde{π}$ 사이의 거리 측정으로 대체하고, 상수 $ϵ$를 적절히 변경함으로써 가능하다.

**Total Variation Divergence**

$$
D_{\text{TV}}(p \parallel q) = \frac{1}{2} \sum_i |p_i - q_i|
$$

$$
D^{\text{max}}_{\text{TV}}(\pi, \tilde{\pi}) = \max_s D_{TV}(\pi(\cdot|s) \parallel \tilde{\pi}(\cdot|s)). \quad (7)
$$

**Theorem 1.** $Let \: \alpha = D_{\text{TV}}^{\text{max}}(\pi_{\text{old}}, \pi_{\text{new}})$, *Then the following bound holds:*

$$
\begin{align*}
\eta(\pi_{\text{new}}) &\geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} \alpha^2 \\
\text{where } & \epsilon = \max_{s,a} |A_{\pi}(s, a)| \tag{8}
\end{align*}
$$

$$
D_{TV}(p \| q)^2 \leq D_{KL}(p \| q)
$$

* *Pollard (2000, Ch. 3)*

$$
D^{\text{max}}_{\text{KL}}(\pi, \tilde{\pi}) = \max_s D_{KL}(\pi(\cdot|s) \| \tilde{\pi}(\cdot|s))
$$

$$
\begin{align*}
\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) & - C D^{\text{max}}_{KL}(\pi, \tilde{\pi}), \\ \text{where } C & = \frac{4\epsilon\gamma}{(1 - \gamma)^2}. \tag{9}
\end{align*}
$$

**Theorem 1.** 정리

$$
\begin{align*}
\eta(\tilde{\pi})& \geq L_{\pi}(\tilde{\pi}) - \frac{2\epsilon\gamma}{(1-\gamma)^2} \alpha^2,
\quad \text{where } \epsilon = \max_s \left| \mathbb{E}_{a\sim\pi'}(a|s) [A_{\pi}(s,a)] \right|.\\

& \geq L_{\pi}(\tilde{\pi}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} \alpha^2, \quad \text{where } \epsilon = \max_{s,a} |A_{\pi}(s, a)|, \: \alpha = D_{\text{TV}}^{\text{max}}(\pi, \tilde{\pi}) \\

& \geq L_{\pi}(\tilde{\pi}) - C D^{\text{max}}_{KL}(\pi, \tilde{\pi}), \quad \text{where } C = \frac{4\epsilon\gamma}{(1 - \gamma)^2}.
\quad (\because D_{TV}(p \| q)^2 \leq D_{KL}(p \| q))
\end{align*}
$$

**Minorization-Maximization(MM) Algorithm** - *(Hunter & Lange, 2004)*

$$
M_i(\pi) = L_{\pi_i}(\pi) - C D^{\text{max}}_{KL}(\pi_i, \pi)
$$

$$
\begin{align*}
& \eta(\pi_{i+1}) \geq M_i(\pi_{i+1}) \quad (\because \eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) - C D^{\text{max}}_{KL}(\pi, \tilde{\pi})) \\
& \eta(\pi_i) = M_i(\pi_i), \text{ therefore, } \\
& \eta(\pi_{i+1}) - \eta(\pi_i) \geq M_i(\pi_{i+1}) - M(\pi_i). \tag{10}
\end{align*}
$$

다르게 표현

$$
\begin{align*}
\eta(\pi_i) & = M_i(\pi_i) \\
& \leq M_i(\pi_{i+1}) \quad (\because \pi_{i+1}=\underset{\pi}{\arg\max}M_i(\pi))\\
& \leq \eta(\pi_{i+1}) \quad (\because \eta(\pi) \geq M_i(\pi))
\end{align*}
$$

## Optimization of Parameterized Policies

> Since we consider parameterized policies $\pi_{\theta}(a \mid s)$ with parameter vector $\theta$, we will overload our previous notation to use functions of $\theta$ rather than $\pi$.

* $\eta(\theta) := \eta(\pi_\theta)$
* $L_{\theta}(\tilde{\theta}) := L_{\pi_{\theta}}(\pi_{\tilde{\theta}})$
* $D_{\text{KL}}(\theta \mid \mid \tilde{\theta}) := D_{\text{KL}}(\pi_{\theta} \| \pi_{\bar{\theta}})$
* $\theta_{\text{old}}$ : previous policy paramters that we want to improve upon.

$$
\underset{\theta}{\text{maximize}} \left [ L_{\theta_{\text{old}}}(\theta) - C D_{\text{KL}}^{\text{max}}(\theta_{\text{old}}, \theta) \right ]
$$

$$
\begin{align*}
& \underset{\theta}{\text{maximize }}L_{\theta_{\text{old}}}(\theta) \tag{11}\\
& \text{subject to } D_{\text{KL}}^{\text{max}}(\theta_{\text{old}}, \theta) \leq \delta
\end{align*}
$$

$$
\overline{D}^{\rho}_{\text{KL}}(\theta_1, \theta_2) := \mathbb{E}_{s \sim \rho} \left[ D_{KL}(\pi_{\theta_1}(\cdot | s) \: || \: \pi_{\theta_2}(\cdot | s)) \right].
$$

$$
\begin{align*}
& \underset{\theta}{\text{maximize }} L_{\theta_{\text{old}}}(\theta) \tag{12} \\
& \text{subject to} \; \overline{D}^{\rho_{\theta_{\text{old}}}}_{\text{KL}}(\theta_{\text{old}}, \theta) \leq \delta.
\end{align*}
$$

## Sample-Based Estimation of the Objective and Constraint

Expanding $L_{\theta_{\text{old}}}$ in equation $(12)$

$$
\begin{align*}
& \underset{\theta}{\text{maximize }} \sum_{s} \rho_{\theta_{\text{old}}}(s) \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a)
 \tag{13} \\
& \text{subject to} \; \overline{D}^{\rho_{\theta_{\text{old}}}}_{\text{KL}}(\theta_{\text{old}}, \theta) \leq \delta.
\end{align*}
$$

* Replace $\sum_{s}\rho_{\theta_\text{old}}(s) \left[ ... \right]$ in the objective by the expectation $\frac{1}{1 - \gamma} \mathbb{E}\_{s \sim \rho_{\theta_{\text{old}}}} \left[ ... \right]$
  * $\rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \cdots$
  * state visited frequency가 확률의 모습을 가지려면 $(1-\gamma)$를 곱해준다.
  * maximize시에 상수를 곱한 것은 의미가 없으므로 $\frac{1}{1-\gamma}$를 제거해도 된다.
* Replace the advantage values $A_{\theta_{\text{old}}}$ by the Q-values $Q_{\theta_\text{old}}$ in Equation $(13)$
  * $A(s, a) = Q(s, a) - V(s)$
  * $V(s)$ : policy에 대해 constant 이며 policy에 아무 관계가 없다.
  * maximize시에 사용하는 $\theta$는 변하지 않으므로 $A$ → $Q$ 가능하다.
* Replace the sum over the actions by an importance sampling estimator.
  * New policy 사용 ⇒ Monte Carlo Sampling 불가 ⇒ Importance Sampling 사용하여 해결
    * New policy에 대한 sample이 없으므로 최대한 $\pi_\theta$를 $\pi_{\theta_{\text{old}}}$로 바꿔야 현재 current policy에서 주어진 샘플들을 사용할 수 있다.
  * Using $q$ to denote the sampling distribution, the contribution of a single $s_n$ to the loss function is
    * $\sum_a \pi\_\theta(a \mid s_n) A_{\theta_{\text{old}}}(s_n, a) = \mathbb{E}\_{a \sim q} \left[ \frac{\pi_\theta(a \mid s_n)}{q(a \mid s_n)} A_{\theta_{\text{old}}}(s_n, a) \right].$

$$
\begin{align*}
\sum_{s} \rho_{\theta_{\text{old}}}(s) \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a) & ⇒ \frac{1}{1-\gamma} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a) \right] \\

& ⇒ \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ \sum_{a} \pi_{\theta}(a|s) A_{\theta_{\text{old}}}(s,a) \right] \\

& ⇒ \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ \mathbb{E}_{a \sim q} \left[ \frac{\pi_{\theta}(a|s)}{q(a|s)} A_{\theta_{\text{old}}}(s,a) \right] \right] \\

& ⇒ \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim q} \left[ \frac{\pi_{\theta}(a|s)}{q(a|s)} Q_{\theta_{\text{old}}}(s,a) \right]
\end{align*}
$$

Our optimization problem in Equation $(13)$ is exactly equivalent to the following one, written in terms of expectations:

$$
\begin{align*}
\underset{\theta}{\text{maximize }} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim q} \left[ \frac{\pi_\theta(a|s)}{q(a|s)} Q_{\theta_{\text{old}}}(s, a) \right] \tag{14} \\
\text{subject to } \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ D_{\text{KL}}(\pi_{\theta_\text{old}}(\cdot|s) \: || \: \pi_\theta(\cdot|s)) \right] \leq \delta.
\end{align*}
$$

## Parctical Algorithm

$$
\frac{1}{N} \sum_{n=1}^{N} \frac{\partial^2}{\partial \theta_i \partial \theta_j} D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot|s_n) \: || \: \pi_{\theta}(\cdot|s_n)),
$$

$$
\frac{1}{N} \sum_{n=1}^{N} \frac{\partial}{\partial \theta_i} \log \pi_{\theta}(a_n|s_n) \frac{\partial}{\partial \theta_j}\log \pi_{\theta}(a_n|s_n).
$$

## Connections with Prior Work

$$
\begin{align*}
& \underset{\theta}{\text{maximize}}  \left[ \nabla_{\theta} L_{\theta_{\text{old}}}(\theta) |_{\theta=\theta_{\text{old}}} \cdot (\theta - \theta_{\text{old}}) \right] \tag{17} \\
& \text{subject to } \frac{1}{2} (\theta_{\text{old}} - \theta)^T A(\theta_{\text{old}}) (\theta_{\text{old}} - \theta) \leq \delta, \\
& \text{where } A(\theta_{\text{old}})_{ij} = \\
& \frac{\partial}{\partial \theta_i}\frac{\partial}{\partial \theta_j} \mathbb{E}_{s \sim \rho_{\pi}} \left[ D_{KL} \left( \pi( \cdot | s, \theta_{\text{old}}) \: || \: \pi( \cdot | s, \theta) \right) \right] |_{\theta=\theta_{\text{old}}}
\end{align*}
$$

$$
\theta_{\text{new}} = \theta_{\text{old}} + \frac{1}{\lambda} A(\theta_{\text{old}})^{-1} \nabla_{\theta} L(\theta) \mid_{\theta=\theta_{\text{old}}}
$$

We can also obatin the standard policy gradient update by using an $\ell_2$ constraint or penalty:

$$
\begin{align*}
&\underset{\theta}{\text{maximize}} \left[ \nabla_{\theta} L_{\text{old}}(\theta) \mid_{\theta=\theta_{\text{old}}} \cdot (\theta - \theta_{\text{old}}) \right] \tag{18}\\
&\text{subject to } \frac{1}{2} \|\theta - \theta_{\text{old}}\|^2 \leq \delta.
\end{align*}
$$

## Appendix

### A. Proof of Policy Improvement Bound

(앞부분이 위에 서술한 내용과 중복되는데 표기의 통일과 설명을 위해 그냥 서술한다)

> Our proof relies on the notion of coupling, where we jointly define the policies $π$ and $π'$ so that they choose the same action with high probability $= (1 − α)$. Surrogate loss $L_\pi(\tilde{\pi})$ accounts for the the advantage of $\tilde{\pi}$ the first time that it disagrees with $π$, but not
subsequent disagreements. Hence, the error in $L_π$ is due to two or more disagreements between $π$ and $\tilde{\pi}$, hence, we get an $O(α^2)$ correction term, where $α$ is the probability of disagreement.

**Lemma 1.** *Given two policies* $\pi$, $\tilde{\pi}$,

$$
\eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{\tau \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_\pi (s_t, a_t) \right] \tag{19}
$$

*This expectation is taken over trajectories $\tau := (s_0, a_0, s_1, a_1, ...)$, and the notation $\mathbb{E}_{\tau \sim \tilde{\pi}}\left[ ... \right]$ indicates that actions are sampled from $\tilde{\pi}$ to generate $\tau$.*

*Proof.* First note that $A_\pi(s,a)=\mathbb{E}\_{s' \sim P(s' \mid s, a)}\left[ r(s)+\gamma V_{\pi}(s')-V_{\pi}(s) \right]$

$$
\begin{align*}
&\mathbb{E}_{\tau \mid \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_\pi (s_t, a_t) \right] \tag{20} \\
& = \mathbb{E}_{\tau \mid \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t ( r(s_t) + \gamma V_\pi (s_{t+1}) - V_\pi (s_t) ) \right] \tag{21} \\
& = \mathbb{E}_{\tau \mid \tilde{\pi}} \left[ - V_\pi (s_0) + \sum_{t=0}^{\infty} \gamma^t r(s_t) \right] \tag{22} \\
& = -\mathbb{E}_{s_0} [V_\pi (s_0)] + \mathbb{E}_{\tau \mid \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t) \right] \tag{23} \\
& = -\eta(\pi) + \eta(\tilde{\pi}) \tag{24}
\end{align*}
$$

여기까지는 위에서 했던 부분인데 아래부터 새로운 표기가 등장한다.

Define $\bar{A}(s)$ to be the expected advantage of $\tilde{\pi}$ over $\pi$ at state $s$:

$$
\bar{A}(s)=\mathbb{E}_{a \sim \tilde{\pi}(\cdot \mid s)} \left[ A_\pi(s,a) \right]. \tag{25}
$$

Now Lemma 1 can be written as follows:

$$
\eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{\tau \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t \bar{A}(s_t) \right] \tag{26}
$$

Note that $L_\pi$ can be written as

$$
L_\pi (\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t \bar{A}(s_t) \right] \tag{27}
$$

(참고로 논문(위)에서는 $L_\pi(\tilde{\pi})$를 아래와 같이 나타내었다.)

$$
L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{3}
$$

State가 $\pi$, $\tilde{\pi}$ 중에서 어느 정책을 따랐는지가 $(26)$, $(27)$의 차이이다. $\eta(\tilde{\pi})$와 $L_\pi(\tilde{\pi})$ 사이의 차이를 제한(bound)하기 위해, 각 타임스텝에서 발생하는 차이를 제한하자. 이를 위해 $\pi$와 $\tilde{\pi}$가 얼마나 일치하는지를 나타내는 척도를 도입한다. 구체적으로, 우리는 정책들을 *couple*하여 행동 쌍(pair of actions)에 대한 joint distribution을 정의한다.
