---
layout: single
title: "TRPO, Trust Region Policy Optimization"
date: 2023-10-29 15:43:54
lastmod: 2023-10-29 15:43:52
categories: RL
tag: [RL, TRPO]
toc: true
toc_sticky: true
use_math: true
---

(수정중... 지속적으로 수정될 예정)

$\eta(\pi)$ : expected discounted reward.

$$\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0, a_0, \cdots, \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t}A_{\pi}(s_t, a_t) \right ] \tag{1}$$
* *Kakade & Langford (2002)*
* $s_0, a_0, \cdots,=\tau$


$$
\begin{aligned}
    \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t}A_{\pi} (s_t, a_t) \right ] & = \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t} (r(s_t) + \gamma V_{\pi}(s_{t+1})-V_{\pi}(s_t)) \right ] \\
    & = \eta(\tilde{\pi}) + \mathbb{E}_{\tau \sim \tilde{\pi}} \left [ \sum_{t=0}^{\infty} \gamma^{t+1} V_{\pi}(s_{t+1}) - \sum_{t=0}^{\infty}\gamma^{t}V_{\pi}(s_t) \right ] \\
    & = \eta(\tilde{\pi}) - \mathbb{E}_{s_0} \left [ V_{\pi} (s_0) \right ] \\
    & = \eta(\tilde{\pi}) - \eta(\pi)
\end{aligned}
$$

$$
\rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \cdots
$$


$$
\begin{align}
\eta(\tilde{\pi}) &= \eta(\pi) + \sum_{t=0}^{\infty} \sum_s P(s_t = s|\tilde{\pi}) \sum_a \tilde{\pi}(a|s)\gamma^t A_\pi(s,a) \notag \\
&= \eta(\pi) + \sum_{t=0}^{\infty} \sum_s \gamma^t P(s_t = s|\tilde{\pi}) \sum_a \tilde{\pi}(a|s)A_\pi(s,a) \notag \\
&= \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{2}
\end{align}
$$

$$\rho_{\tilde{\pi}} \rightarrow \rho_{\pi}$$

$$
L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s)A_\pi(s,a). \tag{3}
$$

$$
\begin{align*}
A(s, a) & = Q(s, a) - V(s) \\
& = Q(s, a) - \mathbb{E}_a \left [ Q(s, a) \right ]
\end{align*}
$$
* $\mathbb{E}_a$ : action 에 대한 expectation

$$
\begin{align*}
\mathbb{E}_a \left [ A(s, a) \right ] & = Q(s, a) - V(s) \\
& = \mathbb{E}_a \left [ Q(s, a) - \mathbb{E}_a \left [ Q(s, a) \right ]\right ] \\
& = 0 \quad \text{(same policy)}
\end{align*}
$$

* 같은 policy에 대해서 위 식은 0과 같다.

$$
\begin{align*}
L_{\pi_{\theta_0}} (\pi_{\theta_0}) &= \eta(\pi_{\theta_0}),\\
\nabla_{\theta} L_{\pi_{\theta_0}} (\pi_{\theta})\big|_{\theta=\theta_0} &= \nabla_{\theta} \eta(\pi_{\theta})\big|_{\theta=\theta_0}. \tag{4}
\end{align*}
$$

$$
\because L_{\pi_{\theta_0}}(\pi_{\theta_0}) = \eta(\pi_{\theta_0}) + \sum_s \rho_{\pi_{\theta_0}} \sum_a \pi_{\theta_0}(a|s)A_{\pi_{\theta_0}}(s,a).
$$
* *Kakade & Langford (2002)*

---

$$\pi_\text{new}(a|s)=(1 - \alpha)\pi_{\text{old}}(a|s) + \alpha \pi' (a|s) \tag{5}$$
* *Kakade & Langford (2002)*
* $\pi_\text{old}$ : current policy
* $\pi'=\arg \max_{\pi'}L_{\pi_\text{old}}(\pi')$
* $\alpha$ 비율 만큼만 new policy

$$
\begin{align*}
\eta(\pi_{\text{new}}) & \geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{2\epsilon\gamma}{(1-\gamma)^2} \alpha^2, \\
& \text{where } \epsilon = \max_s \left| \mathbb{E}_{a\sim\pi'}(a|s) [A_{\pi}(s,a)] \right|. \tag{6}
\end{align*}
$$

* *Kakade & Langford (2002)*
* $\alpha$ ↑ ⇒ 더 많은 new policy ⇒ 오차 ↑ ⇒ lower bound ↓
* $\gamma$ ↑ ⇒ time step이 커질수록 discount 양 ↓ ⇒ lower bound ↓

Our principal theoretical result is that the policy improvement bound in Equation (6) can be extended to general stochastic policies, rather than just mixture polices, by replacing $α$ with a distance measure between $π$ and $\tilde{π}$, and changing the constant $ϵ$ appropriately.

우리의 주요 이론적 결과는 식(6)의 policy improvement bound가 mixture policy뿐만 아니라 일반 확률적 정책으로 확장될 수 있음을 보여준다. $α$를 $π$와 $\tilde{π}$ 사이의 거리 측정으로 대체하고, 상수 $ϵ$를 적절히 변경함으로써 가능하다.

**Total Variation Divergence**

$$
D_{\text{TV}}(p \parallel q) = \frac{1}{2} \sum_i |p_i - q_i|
$$

$$
D^{\text{max}}_{\text{TV}}(\pi, \tilde{\pi}) = \max_s D_{TV}(\pi(\cdot|s) \parallel \tilde{\pi}(\cdot|s)). \quad (7)
$$

**Theorem 1.** $Let \: \alpha = D_{\text{TV}}^{\text{max}}(\pi_{\text{old}}, \pi_{\text{new}})$, *Then the following bound holds:*

$$
\begin{align*}
\eta(\pi_{\text{new}}) &\geq L_{\pi_{\text{old}}}(\pi_{\text{new}}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} \alpha^2 \\
\text{where } & \epsilon = \max_{s,a} |A_{\pi}(s, a)| \tag{8}
\end{align*}
$$

$$
D_{TV}(p \| q)^2 \leq D_{KL}(p \| q)
$$

* *Pollard (2000, Ch. 3)*

$$
D^{\text{max}}_{\text{KL}}(\pi, \tilde{\pi}) = \max_s D_{KL}(\pi(\cdot|s) \| \tilde{\pi}(\cdot|s))
$$

$$
\begin{align*}
\eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) & - C D^{\text{max}}_{KL}(\pi, \tilde{\pi}), \\ \text{where } C & = \frac{4\epsilon\gamma}{(1 - \gamma)^2}. \tag{9}
\end{align*}
$$

**Theorem 1.** 정리

$$
\begin{align*}
\eta(\tilde{\pi})& \geq L_{\pi}(\tilde{\pi}) - \frac{2\epsilon\gamma}{(1-\gamma)^2} \alpha^2,
\quad \text{where } \epsilon = \max_s \left| \mathbb{E}_{a\sim\pi'}(a|s) [A_{\pi}(s,a)] \right|.\\

& \geq L_{\pi}(\tilde{\pi}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} \alpha^2, \quad \text{where } \epsilon = \max_{s,a} |A_{\pi}(s, a)|, \: \alpha = D_{\text{TV}}^{\text{max}}(\pi, \tilde{\pi}) \\

& \geq L_{\pi}(\tilde{\pi}) - C D^{\text{max}}_{KL}(\pi, \tilde{\pi}), \quad 
\text{where } C = \frac{4\epsilon\gamma}{(1 - \gamma)^2}.
\quad (\because D_{TV}(p \| q)^2 \leq D_{KL}(p \| q))
\end{align*}
$$

**Minorization-Maximization(MM) Algorithm** - *(Hunter & Lange, 2004)*

$$
M_i(\pi) = L_{\pi_i}(\pi) - C D^{\text{max}}_{KL}(\pi_i, \pi)
$$

$$
\begin{align*}
& \eta(\pi_{i+1}) \geq M_i(\pi_{i+1}) \quad (\because \eta(\tilde{\pi}) \geq L_{\pi}(\tilde{\pi}) - C D^{\text{max}}_{KL}(\pi, \tilde{\pi})) \\
& \eta(\pi_i) = M_i(\pi_i), \text{ therefore, } \\
& \eta(\pi_{i+1}) - \eta(\pi_i) \geq M_i(\pi_{i+1}) - M(\pi_i). \tag{10}
\end{align*}
$$

다르게 표현

$$
\begin{align*}
\eta(\pi_i) & = M_i(\pi_i) \\
& \leq M_i(\pi_{i+1}) \quad (\because \pi_{i+1}=\underset{\pi}{\arg\max}M_i(\pi))\\
& \leq \eta(\pi_{i+1}) \quad (\because \eta(\pi) \geq M_i(\pi))
\end{align*}
$$