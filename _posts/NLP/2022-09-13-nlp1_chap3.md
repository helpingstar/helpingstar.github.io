---
layout: single
title: "자연어 처리 개요"
date: 2022-09-13 13:33:11
lastmod : 2022-09-13 13:33:14
categories: NLP
tag: [NLP, 텐서플로2와 머신러닝으로 시작하는 자연어 처리]
toc: true
toc_sticky: true
use_math: true
---

# 단어 표현

**단순 이진화한 문자**

ex) '언' : 1100010110111000

언어적인 특성이 전혀 없기에 자연어 처리에는 적합하지 않다

**단어 표현**(word Representation)

`단어 임베딩(word embedding)`, `단어 벡터(word vector)`로 표현하기도 한다.

언어적인 특성을 반영해서 단어를 수치화하는 방법을 찾는 것

**원-핫 인코딩**(one-hot encoding)

각 단어의 인덱스를 정한 후 각 단어의 벡터에서 그 단어에 해당하는 인덱스의 값을 1로 표현하는 방식

문제점

* 실제에서는 단어가 너무 많아 벡터가 너무 커짐
* 그에비해 사용하는 값은 1이 되는 값 하나여서 비효율
* 단어만 알려주고 벡터값 자체는 단어의 의미, 특성 표현불가

**분포 가설**

비슷한 위치에 나오는 단어는 비슷한 의미를 가진다.

`count-based method`, `predictive method`가 있다.

**count-based method**

어떤 글의 문맥 안에 단어가 동시에 등장(Co-occurrence)하는 횟수를 센다. 동시 등장 횟수를 하나의 행렬로 나타낸 뒤 그 행렬을 수치화해서 단어 벡터로 만드는 방법을 사용한다.

* 특이값 분해(Singular Value Decomposition, SVD)
* 잠재의미분석(Latent Semnatic Analysis, LSA)
* Hyperspace Analogue to Language(HAL)
* Hellinger PCA(Principa Component Analysis)

모두 동시 출현 행렬(Co-occurrence Matrix)을 만들고 그 행렬들을 변형하는 방식

장점
* 빠르다, 단어 벡터 ↑, 시간 ↑, but 적은 시간으로 단어 벡터를 만들 수 있다
* 데이터가 많을 경우 단어가 잘 표현되고 효율적

**predictive method**

신경망 구조 혹은 어떠한 모델을 사용해 특정 문맥에서 어떤 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식

* Word2vec
* NNLM(Neural Network Language Model)
* RNNLM(Recurrent Neural Network Language Model)

`Word2Vec`의 모델
* CBOW(Continuous Bag of Words)
  * 어떤 단어를 문맥 안의 주변 단어들을 통해 예측
* Skip-Gram
  * 어떠 단어를 가지고 특정 문맥 주변 단어들을 예측

![cbow_skipgram](../../assets/images/NLP/cbow_skipgram.png){: width="80%" height="80%" class="align-center"}
[source](https://arxiv.org/pdf/1309.4168v1.pdf) : Exploiting Similarities among Languages for Machine Translation


`CBOW`의 학습 순서

1. 각 주변 단어들을 원-핫 벡터로 만들어 입력값으로 사용(입력층 벡터)
2. 가중치 행렬(weight matrix)을 각 원-핫 벡터에 곱해서 n-차원 벡터를 만든다(N-차원 은닉층)
3. 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터를 만든다(출력 벡터)
4. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다.
5. 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교해서 학습한다.

`Skip-Gram`의 학습 순서

1. 하나의 단어를 원-핫 벡터로 만들어서 입력값으로 사용(입력층 벡터)
2. 가중치 행렬을 원-핫 벡터에 곱해서 n-차원 벡터를 만든다(N-차원 은닉층)
3. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다(출력층 벡터)
4. 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 원-핫 벡터와 비교해서 학습한다.


| 차이       | 입력값        | 학습         |
|-----------|-------------|------------|
| CBOW      | 여러 개의 단어 사용 | 하나의 단어와 비교 |
| Skip-Gram | 하나의 단어 사용   | 여러 단어와 비교  |

`Word2Vec` 모델의 장점

* 단어 간의 유사도를 잘 측정
* 단어들의 복잡한 특징까지도 잘 잡아낸다.

보통 `Skip-Gram`, `Predictive Method`가 성능이 좋다. `count-based method`와 `predictive method` 방법을 모두 포함하는 `Glove` 또한 자주 사용된다.

# Text Classification

**예시**
* 스팸 분류, (스팸/일반)
* 감정 분류, (긍정/.../부정) ex) 영화 리뷰

> 출처
 - 전창욱, 최태균, 조중현, 신성진,『텐서플로2와 머신러닝으로 시작하는 자연어 처리』, 위키북스(2020)