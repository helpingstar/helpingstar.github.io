---
layout: single
title: "Variational Auto-Encoder (VAE)"
date: 2024-06-04 06:14:32
lastmod: 2024-06-04 06:14:32
categories: DL
tag: [AutoEncoder, VAE]
toc: true
toc_sticky: true
use_math: true
published: false
---

## 참고자료
* [1] [오승상 딥러닝 Deep Learning 36 Variational AE 1](https://youtu.be/yfxytn0OAAg?si=Fpdoyeuzg-8JNTnJ)
* [2] [오승상 딥러닝 강의자료](https://sites.google.com/view/seungsangoh)

# Variational Auto-Encoder (VAE)

![VAE_01](../../assets/images/dl/VAE_01.png){: width="80%" height="80%" class="align-center"}
<p style="text-align: center; font-style: italic;"> (image source : 오승상 딥러닝 강의자료 p.154) </p>

* 새로운 모델을 생성할 수 있는 generative model이다.
* 하나의 latent vector를 만들어내는 것이 아니라 mean  $\mu$, standard deviation $\sigma$ 2개의 latent vector를 만들어낸다. 궁극적으로 그것을 통해 Gaussian distribution $N(\mu, \sigma^2)$를 만들낸다.
* AE에서는 각 attribute마다 single value를 만들면 된다. VAE에서는 각 attribute마다 posterior distribution $p(z \vert x)$를 만들려 한다.
  * input $x$이 주어졌을 때 $z$값에 대한 분포를 만들려고 한다. 하지만 posterior distribution을 직접 계산하는 것은 불가능에 가깝다.
  * input에 대해서 input들을 잘 표현할 수 있는 latent vector의 attribute(z값)들이 input값들을 잘 표현하기 위해서는 z에 대한 분포가 복잡할 수밖에 없다.(이것을 찾는 것은 불가능)
  * 따라서 gaussian으로 approximation하게 된다.(Variational inference)
* latent space 를 distribution으로 표현하게 되면 해당 distribution에서 sampling이 가능해진다. (해당 분포를 따르는 샘플 여러개를 만들어낼 수 있다.)
* decoder는 sample들에 대해서 reconstruct하게 된다.
* 이럴 경우 한 input에 대해 각기 다른 여러 개의 reconstructed output이 나온다.
* VAE의 목적은 input과 유사한 많은 데이터들을 생성할 수 있게 하는 것이다.
* (probabilistic encoder + generative decoder)

기존의 새로운 데이터를 만들기 위한 방법

* input data 변화 (data augmentation)
  * 픽셀 단위의 변화 (ex. crop, zoom, rotation, ...)
* AE의 latent vector 변화
  * feature의 변화 (ex. 사람 얼굴에서 age, gender, ... 등의 feature를 변경)
  * 어떤 값을 어떻게 얼마나 바꿔야할지를 알 수 없어 위험한 방법이다.

![VAE_02](../../assets/images/dl/VAE_02.png){: width="80%" height="80%" class="align-center"}
<p style="text-align: center; font-style: italic;"> (image source : 오승상 딥러닝 강의자료 p.155) </p>

* single point z를 만들어내는 것이 아니라 posterior distribution을 만들어내고 싶어한다. 하지만 이는 단순한 분포가 아니기에 상당히 복잡하다(intractable). 따라서 정규분포 variational posteriror $q_\theta(z \vert x)$로 근사한다(Variational Inference).
* normal distribution을 approximation 한다는 것은 평균과 표준편차를 찾아낸다는 뜻이다. 우리는 네트워크 $\theta$를 통해 평균 $\mu$와 표준편차 $\sigma$를 찾는다.
* probabilistic encoder $q_\theta$를 통과한 x는 latent distribution을 만들어내는데 이것은 $N(\mu_x, \sigma_x^2)$을 나타낸다. 그렇다면 encoder가 $\mu_x, \sigma_x$를 output으로 꺼내면 된다. 이 분포에서 $z \sim N(\mu_x, \sigma_x)$를 sampling하고 샘플링된 값 기반의 latent vector를 사용해서 input reconstruction을 한다.
* VAE의 latent space는 feature마다 확률분포를 나타내는 space를 가진다.
* input x가 encoder를 통과하면 해당 input에 대한 평균 $\mu$와 표준편차 $\sigma$를 만들어내는데 평균과 표준편차는 직접 계산하는 것이 아니라 평균과 표준편차를 나타내는 값을 네트워크가 만들어낸다.
* 평균과 표준편차가 나타내는 Normal Distribution $N(\mu_x, \sigma_x^2)$이 posteiror distribution $p(z \vert x)$를 잘 approximation할 수 있도록 평균과 표준편차를 만들어내도록 학습한다. 즉 데이터의 평균과 표준편차를 직접 계산하지 않는다.

encoder가 단순히 latent space를 만들기 위해 $\mu, \sigma$를 출력하게 하는 것은 잘 작동하지 않는다. 우리는 $N(\mu, \sigma^2)$에서 샘플링 된 값으로 decoder를 통과시켜 output을 만든다. 그런데 이 output이 input과 최대한 유사하게 하게 만들려고 한다. 그렇다면 $\mu$는 기존 AE의 z값과 같은 값으로 바로 만들려고 하고 편차 $\sigma$는 0으로 만든다. 이 때 output은 input과 유사하게 된다.

하지만 우리의 목적은 latent space의 distribution을 통해서 sampling을 다양하게 해서 여러가지 데이터를 생성하려는 것이 목적이다. 하지만 표준편차가 0이 되면 거의 z값과 비슷한 값만 나오기 때문에 sampling이 의미가 없어진다.(=다양한 output을 만들 수 없다.) 따라서 표준편차 $\sigma$의 값을 어느 정도 이상으로 유지하기 위해 기존의 Loss function에 다른 요소가 더 붙는다.

**목적**
* reconstruction error $L(\hat{y}, x) = \frac{1}{2}\sum_i(\hat{y}_i - x_i)^2$ 최소화 (기존 AutoEncoder의 Loss function)
* 표준편차 $\sigma$가 0으로 가는 것을 막기 위해, $q_{\theta}(z \vert x) \sim N(\mu_x, \sigma_x^2)$의 $N(\mu_x, \sigma_x^2)$ 분포가 $N(0, 1)$을 따르도록 한다. 이를 regularization term이라고 한다. 이것은 latent layer에서만 적용된다. 따라서 regularity를 만족하게 된다.

**Loss function**
$$L(\hat{y}, x) + \beta D_{\mathrm{KL}}(N(\mu_x, \sigma_x^2) \| N(0, I))$$

**Kullback-Leibler divergence**

$$\begin{align*}
&D_{\mathrm{KL}}(P \ \| \ Q) = \int_x p(x) \log \frac{p(x)}{q(x)}dx = \mathbb{E}_{x \sim P} \left[ \log \frac{p(x)}{q(x)} \right]   
\end{align*}$$

* 두 확률분포 $P, Q$가 비슷할 수록 KL Divergence가 낮아진다.
* $N(\mu_x, \sigma_x^2)$가 $N(0, I)$를 따르는 분포와 최대한 유사해지도록 한다.
  * 따라서 latent space의 regularity가 좋아지게 된다.
* (Reconstruction term) + (Regularization term)
  * $\beta$를 통해 두 term의 tradeoff를 조절한다.

**Implementation**
* $L(\hat{y}, x)$를 볼 때 AE에서는 $\hat{y}$를 계산하기 위해 differentiable function으로만 계산 했지만 VAE에서는 latent distribution에서 sampled latent vector를 얻기 위한 random sampling 과정이 포함된다. 하지만 random sampling은 differentiable function이 아니기에 backpropagation을 직접 사용할 수 없다.
* 따라서 위 문제를 해결하기 위해 reparameterization trick을 사용한다.
  * 원칙은 $N(\mu_x, \sigma_x^2)$를 사용하는 것이다. 하지만 여기서 직접 하지 않고 $N(0, I)$ 에서 $\epsilon$을 sampling한다. 그리고 sampled latent vector를 $z = \mu + \sigma \odot \epsilon$와 같이 계산한다.
    * $\epsilon$ : $N(0, I)$에서 sampling한 것이다.
    * $\sigma \odot \epsilon$ : 평균이 0이고 편차가 $\sigma$
    * $\mu + \sigma \odot \epsilon$ : 평균이 $\mu$, 편차가 $\sigma$인 분포
  * 즉 $\mu + \sigma \odot \epsilon$은 $N(\mu, \sigma)$에서 sampling한 것과 동일하다.